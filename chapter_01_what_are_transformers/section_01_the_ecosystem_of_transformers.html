<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>1.1. Transformer的生态系统 &#8212; Transformer for NLP 0.0.1 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/d2l.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="1.2. 使用Transformer优化NLP模型" href="section_02_optimizing_nlp_models_with_transformers.html" />
    <link rel="prev" title="1. 什么是Transformer?" href="index.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="index.html"><span class="section-number">1. </span>什么是Transformer?</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active"><span class="section-number">1.1. </span>Transformer的生态系统</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_01_what_are_transformers/section_01_the_ecosystem_of_transformers.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <span class="title-text">
                  Transformer for NLP
              </span>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">1. 什么是Transformer?</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">1.1. Transformer的生态系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_02_optimizing_nlp_models_with_transformers.html">1.2. 使用Transformer优化NLP模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_03_what_resources_should_we_use.html">1.3. 我们需要什么资源</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/index.html">2. 从Transformer的架构开始</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_01_the_rise_of_the_transformer_attention_is_all_you_need.html">1. Transformer模型架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_02_training_and_performance.html">2. 模型训练和表现</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_03_transformer_models_in_hugging_face.html">3. Hugging Face上的Transformer模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_04_exercise.html">4. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/index.html">2. 微调BERT模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/section_01_the_architecture_of_bert.html">2.1. BERT模型的架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/section_02_fine_tuning_bert.html">2.2. 微调BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/section_03_exercise.html">2.3. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_04_pretraining_a_roberta_model_from_scratch/index.html">3. 从头开始预训练RoBERTa模型</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_04_pretraining_a_roberta_model_from_scratch/index.html#id13">4. 练习</a></li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <span class="title-text">
                  Transformer for NLP
              </span>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">1. 什么是Transformer?</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">1.1. Transformer的生态系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_02_optimizing_nlp_models_with_transformers.html">1.2. 使用Transformer优化NLP模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_03_what_resources_should_we_use.html">1.3. 我们需要什么资源</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/index.html">2. 从Transformer的架构开始</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_01_the_rise_of_the_transformer_attention_is_all_you_need.html">1. Transformer模型架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_02_training_and_performance.html">2. 模型训练和表现</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_03_transformer_models_in_hugging_face.html">3. Hugging Face上的Transformer模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_04_exercise.html">4. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/index.html">2. 微调BERT模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/section_01_the_architecture_of_bert.html">2.1. BERT模型的架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/section_02_fine_tuning_bert.html">2.2. 微调BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/section_03_exercise.html">2.3. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_04_pretraining_a_roberta_model_from_scratch/index.html">3. 从头开始预训练RoBERTa模型</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_04_pretraining_a_roberta_model_from_scratch/index.html#id13">4. 练习</a></li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <div class="section" id="transformer">
<h1><span class="section-number">1.1. </span>Transformer的生态系统<a class="headerlink" href="#transformer" title="Permalink to this heading">¶</a></h1>
<p>Transformer模型代表了一种范式变革，它们需要一个新的名称来描述：基座模型（Foundation
Model）。为此，斯坦福大学成立了基座模型研究中心（CRFM）。2021年8月，CRFM发布了一份由100多名科学家和专业人士撰写的长达200页的论文（请参见参考文献部分）：<em>On
the Opportunities and Risks of Foundation
Models</em>（《关于基座模型的机遇与风险》）。</p>
<p>基座模型并非由学术界创建，而是由大型科技公司创建的。例如，谷歌发明了transformer模型，并由此推出了Google
BERT。微软与OpenAI合作推出了GPT-3。这些大型科技公司在基座模型的研发和推广方面发挥了重要作用。</p>
<p>大型科技公司需要找到更好的模型来应对流入其数据中心的以PB计的数据呈指数增长。因此，transformer模型是出于必要性而诞生的。它们应运而生，以应对海量数据的处理需求。</p>
<p>让我们首先考虑工业4.0，以理解为何需要工业化的人工智能。</p>
<div class="section" id="id1">
<h2><span class="section-number">1.1.1. </span>工业4.0<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h2>
<p>农业革命引发了第一次工业革命，引入了机械化。第二次工业革命诞生了电力、电话和飞机。第三次工业革命是数字化。</p>
<p>第四次工业革命，即工业4.0，孕育了无数的机器对机器连接：机器人、机器人、物联网设备、自动驾驶汽车、智能手机、从社交媒体存储中收集数据的机器人等等。</p>
<p>反过来，这些数以百万计的机器和机器人每天生成数十亿条数据记录，包含图像、声音、文字和事件等。</p>
<!-- 图改文 --><p>工业4.0的影响和范畴包含了以下几个方面： - 制造业中机器对机器的连接 -
物流业中车辆到仓库的连接 - 卡车到轮船或飞机的交通连接 -
AI或经典的自动化微决策 -
物联网及所有联网设备、社交媒体中服务器对服务器的连接和处理 -
经典软件和基于基座模型AI的产业化</p>
<p>工业4.0需要能够在人类历史上前所未有的大规模数据面前，处理数据并做出决策的智能算法，而无需人类干预。</p>
<p>大型科技公司需要找到一个单一的人工智能模型，能够执行过去需要多个独立算法完成的各种任务。</p>
</div>
<div class="section" id="id2">
<h2><span class="section-number">1.1.2. </span>基座模型<a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h2>
<p>Transformer模型具有两个鲜明的特点：高度的同质化（Homogenization）和令人惊叹的涌现特性（Emergence
Properties）。同质化使得可以使用一个模型来执行各种各样的任务。这些能力是通过在超级计算机上训练数十亿级参数的模型而出现的。</p>
<!-- 图改文 --><p>范式的改变使得基座模型成为一个后深度学习生态系统，新的AI范式或者说工业4.0时代的AI专家的范畴包括以下几个方面：
- 可以独立完成所有NLP、CV和更多任务的基座模型 -
可以完成一个或几个任务的部分训练transformer模型 - 经典的深度学习任务 -
经典的机器学习算法（LR, KNN, KMC, MDP等） - 专家系统和规则库 -
帮助构建AI流水线的代码编写</p>
<p>基座模型虽然采用了创新的架构，但是它们仍然建立在人工智能的历史之上。因此，人工智能专家的技能范围正在强有力地扩展！</p>
<p>当前的Transformer模型生态系统与人工智能的其他演进方式不同，并可以总结为四方面的特点：</p>
<ul>
<li><p>模型架构（Model Architecture）</p>
<p>Transformer模型是工业级的。模型的每一层是相同的，并且它们专门设计用于并行处理。我们将在“<a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/index.html#chapter-2"><span class="std std-ref">从Transformer的架构开始</span></a>”中详细介绍Transformer的架构。</p>
</li>
<li><p>数据（Data）</p>
<p>大型科技公司拥有人类历史上最庞大的数据来源，这些数据首先由第三次工业革命（数字化）产生，并通过工业4.0的推动而增长到难以想象的规模。</p>
</li>
<li><p>算力（Computing Power）</p>
<p>大型科技公司拥有以前从未见过的规模的计算能力。例如，GPT-3的训练速度约为50P次浮点运算每秒（PFlops/s），而谷歌现在拥有领域特定的超级计算机，其运算速度超过80PFlops/s。</p>
</li>
<li><p>提示工程（Prompt Engineering）</p>
<p>经过高度训练的Transformer模型可以通过提示词来执行各种不同的任务。提示词以自然语言形式输入。然而，所使用的词汇需要一定的结构，使得提示词成为一种元语言，从而出现了基于基座模型的提示工程。</p>
</li>
</ul>
<p>因此，Transformer基座模型是在超级计算机上使用十亿级的数据和十亿级的参数进行训练的Transformer模型。该模型可以在没有进一步微调的情况下执行各种任务。因此，基座模型的规模是独特的。这些完全训练好的模型通常被称为引擎。只有GPT-3、Google
BERT和少数几个Transformer引擎才能够被称为基座模型。</p>
<p>本书中提到的OpenAI的GPT-3或Google的BERT等模型仅代表其基座模型。这是因为GPT-3和Google
BERT在超级计算机上进行了全面训练。尽管其他模型在有限的使用场景中可能很有趣且有效，但由于资源不足，它们无法达到基座模型的同质化水平。</p>
<p>现在让我们来探索一个基座模型是如何工作的，并且它们如何改变了我们开发程序的方式的例子。</p>
</div>
<div class="section" id="nlp">
<h2><span class="section-number">1.1.3. </span>编程是否成为了NLP的一个子领域？<a class="headerlink" href="#nlp" title="Permalink to this heading">¶</a></h2>
<p>Chen等人（2021年）在2021年8月发表了一篇轰动性的论文，介绍了Codex，这是一个能够将自然语言转换为源代码的GPT-3模型。Codex是在5400万个公共GitHub软件仓库上进行训练的。Codex可以生成有趣的自然语言到源代码的转换结果，我们将在第16章“<a class="reference internal" href="../chapter_16_the_emergence_of_transformer_driven_copilots/index.html#chapter-16"><span class="std std-ref">Transformer驱动的Copilot</span></a>”中看到具体示例。</p>
<p>编程是否是已经成为一种将自然语言翻译为源代码语言的机器翻译任务？</p>
<p>编程是否正在成为针对GPT-3引擎的NLP任务？</p>
<p>在回答这些问题之前，让我们先看一个例子。</p>
<p>由于Codex是一种随机算法，我们需要一些技巧来构建元语言。如果不能正确构建提示语（Prompt），可能无法生成所期望的结果。</p>
<p>这里创建了一些用于Codex实验的提示语。这些例子只是为了让读者了解Codex的工作原理，并且纯粹是为了教育目的。提示语如下：</p>
<ul class="simple">
<li><p>在Python中生成一个1到100之间的200个整数的随机分布。</p></li>
<li><p>使用matplotlib绘制数据图表。</p></li>
<li><p>创建一个包含3个中心点的K均值聚类模型，并拟合该模型。</p></li>
<li><p>打印聚类标签。</p></li>
<li><p>绘制聚类结果图表。</p></li>
<li><p>绘制带有中心点的聚类图表。</p></li>
</ul>
<p>Codex自动将这些使用自然元语言编写的提示语翻译成了Python代码！</p>
<p>由于Codex是一种随机模型，所以如果重复进行实验，它可能无法完全复现相同的代码。我们需要通过实验来学习并驾驭这种元语言。</p>
<p>在提供提示语后，Codex会自动生成Python程序，并且可以复制和测试：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>

<span class="c1"># 生成随机分布的整数</span>
<span class="n">random_numbers</span> <span class="o">=</span> <span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">)]</span>

<span class="c1"># 绘制数据图表</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">),</span> <span class="n">random_numbers</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Random Integer Distribution&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Index&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Value&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># 创建K均值聚类模型并拟合数据</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="n">x</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">random_numbers</span><span class="p">])</span>

<span class="c1"># 打印聚类标签</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cluster Labels:&quot;</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

<span class="c1"># 绘制聚类结果图表</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">),</span> <span class="n">random_numbers</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Clustering Results&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Index&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Value&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># 绘制带有中心点的聚类图表</span>
<span class="n">centroids</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">),</span> <span class="n">random_numbers</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">centroids</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Clustering Results with Centroids&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Index&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Value&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>上述程序在Python解释器是可以运行的（前提是需要的库都已安装好）。读者也可以尝试JavaScript等其他语言上的实验。可以在<a class="reference external" href="https://poe.com/AICodeX">poe.com</a>上搜索AICodeX进行简单的尝试。</p>
<p>GitHub
Copilot现在可与一些Microsoft开发工具一起使用，我们将在“<a class="reference internal" href="../chapter_16_the_emergence_of_transformer_driven_copilots/index.html#chapter-16"><span class="std std-ref">Transformer驱动的Copilot</span></a>”中看到。学习提示工程的元语言可以减少利用Copilot来进行开发的时间。</p>
<p>如果终端用户掌握了元语言，他们就可以创建一些软件原型和较小的任务。未来，编程Copilot的功能将会继续扩展。</p>
<p>在第“<a class="reference internal" href="../chapter_16_the_emergence_of_transformer_driven_copilots/index.html#chapter-16"><span class="std std-ref">Transformer驱动的Copilot</span></a>”中，我们将了解Codex在人工智能的未来中的定位。</p>
<p>现在，让我们来展望一下人工智能专家的光明未来吧。</p>
</div>
<div class="section" id="id3">
<h2><span class="section-number">1.1.4. </span>人工智能专家的未来<a class="headerlink" href="#id3" title="Permalink to this heading">¶</a></h2>
<p>基座模型的社会影响不容小觑。提示语的构建已成为人工智能专家所必需的技能。然而，人工智能专家的未来不能仅仅局限于Transformer模型。在工业4.0时代，人工智能和数据科学相互交叉重叠。</p>
<p>人工智能专家将涉及使用传统人工智能、物联网（IoT）、边缘计算等技术进行机器之间算法的开发。人工智能专家还将使用传统算法设计和开发有趣的连接，将机器人、服务器和各种类型的连接设备进行连接。</p>
<p>因此，本书不仅限于提示语构建，还涵盖了成为“工业4.0人工智能专家”或“I4.0人工智能专家”所需的广泛设计技能。</p>
<p>提示语构建是人工智能专家需要开发的设计技能的一部分。在本书中，我将因此将未来的人工智能专家称为“工业4.0人工智能专家”。</p>
<p>现在让我们对Transformer如何优化自然语言处理模型进行一个整体的概览。</p>
</div>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">1.1. Transformer的生态系统</a><ul>
<li><a class="reference internal" href="#id1">1.1.1. 工业4.0</a></li>
<li><a class="reference internal" href="#id2">1.1.2. 基座模型</a></li>
<li><a class="reference internal" href="#nlp">1.1.3. 编程是否成为了NLP的一个子领域？</a></li>
<li><a class="reference internal" href="#id3">1.1.4. 人工智能专家的未来</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="index.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>1. 什么是Transformer?</div>
         </div>
     </a>
     <a id="button-next" href="section_02_optimizing_nlp_models_with_transformers.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>1.2. 使用Transformer优化NLP模型</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>