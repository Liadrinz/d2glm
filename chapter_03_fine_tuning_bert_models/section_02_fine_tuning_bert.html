<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>2. 微调BERT &#8212; Transformer for NLP 0.0.1 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/d2l.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3. 练习" href="section_03_exercise.html" />
    <link rel="prev" title="1. BERT模型的架构" href="section_01_the_architecture_of_bert.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="index.html"><span class="section-number">2. </span>微调BERT模型</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active"><span class="section-number">2. </span>微调BERT</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_03_fine_tuning_bert_models/section_02_fine_tuning_bert.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <span class="title-text">
                  Transformer for NLP
              </span>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_01_what_are_transformers/index.html">1. 什么是Transformer?</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01_what_are_transformers/section_01_the_ecosystem_of_transformers.html">1.1. Transformer的生态系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01_what_are_transformers/section_02_optimizing_nlp_models_with_transformers.html">1.2. 使用Transformer优化NLP模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01_what_are_transformers/section_03_what_resources_should_we_use.html">1.3. 我们需要什么资源</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/index.html">2. 从Transformer的架构开始</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_01_the_rise_of_the_transformer_attention_is_all_you_need.html">2.1. Transformer模型架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_02_training_and_performance.html">2.2. 模型训练和表现</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_03_transformer_models_in_hugging_face.html">2.3. Hugging Face上的Transformer模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_04_exercise.html">2.4. 练习</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">2. 微调BERT模型</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="section_01_the_architecture_of_bert.html">1. BERT模型的架构</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">2. 微调BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_03_exercise.html">3. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_04_pretraining_a_roberta_model_from_scratch/index.html">3. 从头开始预训练RoBERTa模型</a></li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <span class="title-text">
                  Transformer for NLP
              </span>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_01_what_are_transformers/index.html">1. 什么是Transformer?</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01_what_are_transformers/section_01_the_ecosystem_of_transformers.html">1.1. Transformer的生态系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01_what_are_transformers/section_02_optimizing_nlp_models_with_transformers.html">1.2. 使用Transformer优化NLP模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01_what_are_transformers/section_03_what_resources_should_we_use.html">1.3. 我们需要什么资源</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/index.html">2. 从Transformer的架构开始</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_01_the_rise_of_the_transformer_attention_is_all_you_need.html">2.1. Transformer模型架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_02_training_and_performance.html">2.2. 模型训练和表现</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_03_transformer_models_in_hugging_face.html">2.3. Hugging Face上的Transformer模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_04_exercise.html">2.4. 练习</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">2. 微调BERT模型</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="section_01_the_architecture_of_bert.html">1. BERT模型的架构</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">2. 微调BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_03_exercise.html">3. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_04_pretraining_a_roberta_model_from_scratch/index.html">3. 从头开始预训练RoBERTa模型</a></li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <div class="section" id="bert">
<span id="chapter-03-section-02"></span><h1><span class="section-number">2. </span>微调BERT<a class="headerlink" href="#bert" title="Permalink to this heading">¶</a></h1>
<p>本节将对一个BERT模型进行微调，以预测下游任务的可接受性判断，并使用马修斯相关系数（Matthews
Correlation
Coefficient，MCC，后续解释）来衡量预测结果。本节的官方源码<a class="reference external" href="https://github.com/Denis2054/Transformers-for-NLP-2nd-Edition/blob/main/Chapter03/BERT_Fine_Tuning_Sentence_Classification_GPU.ipynb">见此处</a>。</p>
<div class="section" id="id1">
<h2><span class="section-number">2.1. </span>硬件限制<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h2>
<p>Transformer模型需要GPU. 这里建议使用免费的云GPU平台:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://colab.research.google.com/">Google Colab</a></p></li>
<li><p><a class="reference external" href="https://www.kaggle.com/code">Kaggle Notebook</a></p></li>
</ul>
<p>如果使用自己的电脑或自己搭建的服务器上的GPU，请先安装CUDA和cuDNN（Google/问GPT）</p>
</div>
<div class="section" id="pytorchhugging-face-transformer">
<h2><span class="section-number">2.2. </span>安装PyTorch和Hugging Face Transformer<a class="headerlink" href="#pytorchhugging-face-transformer" title="Permalink to this heading">¶</a></h2>
<p>Hugging Face提供了一个预训练的BERT模型。Hugging
Face开发了一个名为PreTrainedModel的基类。通过安装这个类，我们可以从预训练的模型配置中加载一个模型。</p>
<p>Hugging
Face提供了TensorFlow和PyTorch的模块。我建议开发者对这两个环境都有一定的熟悉。优秀的人工智能研究团队可能会使用其中一个或两个环境。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span># Hide outputs
!pip install -q torch transformers
</pre></div>
</div>
</div>
<div class="section" id="id2">
<h2><span class="section-number">2.3. </span>导入模块<a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h2>
<p>我们将导入所需的预训练模块，例如预训练的BERT分词器和BERT模型的配置。同时，我们还导入了AdamW优化器以及序列分类模块：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Hide outputs</span>
<span class="kn">import</span> <span class="nn">io</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">pad_sequences</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="p">(</span><span class="n">DataLoader</span><span class="p">,</span> <span class="n">RandomSampler</span><span class="p">,</span> <span class="n">SequentialSampler</span><span class="p">,</span>
                              <span class="n">TensorDataset</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span><span class="p">,</span> <span class="n">trange</span>  <span class="c1"># for progress bars</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="p">(</span><span class="n">AdamW</span><span class="p">,</span> <span class="n">BertConfig</span><span class="p">,</span> <span class="n">BertForSequenceClassification</span><span class="p">,</span>
                          <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">get_linear_schedule_with_warmup</span><span class="p">)</span>
</pre></div>
</div>
<p>如果一切顺利，不会显示任何消息。Google
Colab已经在我们使用的虚拟机上预先安装了这些模块。</p>
</div>
<div class="section" id="cuda">
<h2><span class="section-number">2.4. </span>指定CUDA作为设备<a class="headerlink" href="#cuda" title="Permalink to this heading">¶</a></h2>
<p>我们现在将指定torch使用CUDA（Compute Unified Device
Architecture）来利用NVIDIA GPU的并行计算能力，用于我们的多头注意力模型：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
!nvidia-smi
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Mon</span> <span class="n">Apr</span> <span class="mi">22</span> <span class="mi">15</span><span class="p">:</span><span class="mi">28</span><span class="p">:</span><span class="mi">45</span> <span class="mi">2024</span>
<span class="o">+---------------------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">NVIDIA</span><span class="o">-</span><span class="n">SMI</span> <span class="mf">545.30</span>                 <span class="n">Driver</span> <span class="n">Version</span><span class="p">:</span> <span class="mf">546.09</span>       <span class="n">CUDA</span> <span class="n">Version</span><span class="p">:</span> <span class="mf">12.3</span>     <span class="o">|</span>
<span class="o">|-----------------------------------------+----------------------+----------------------+</span>
<span class="o">|</span> <span class="n">GPU</span>  <span class="n">Name</span>                 <span class="n">Persistence</span><span class="o">-</span><span class="n">M</span> <span class="o">|</span> <span class="n">Bus</span><span class="o">-</span><span class="n">Id</span>        <span class="n">Disp</span><span class="o">.</span><span class="n">A</span> <span class="o">|</span> <span class="n">Volatile</span> <span class="n">Uncorr</span><span class="o">.</span> <span class="n">ECC</span> <span class="o">|</span>
<span class="o">|</span> <span class="n">Fan</span>  <span class="n">Temp</span>   <span class="n">Perf</span>          <span class="n">Pwr</span><span class="p">:</span><span class="n">Usage</span><span class="o">/</span><span class="n">Cap</span> <span class="o">|</span>         <span class="n">Memory</span><span class="o">-</span><span class="n">Usage</span> <span class="o">|</span> <span class="n">GPU</span><span class="o">-</span><span class="n">Util</span>  <span class="n">Compute</span> <span class="n">M</span><span class="o">.</span> <span class="o">|</span>
<span class="o">|</span>                                         <span class="o">|</span>                      <span class="o">|</span>               <span class="n">MIG</span> <span class="n">M</span><span class="o">.</span> <span class="o">|</span>
<span class="o">|=========================================+======================+======================|</span>
<span class="o">|</span>   <span class="mi">0</span>  <span class="n">NVIDIA</span> <span class="n">GeForce</span> <span class="n">RTX</span> <span class="mi">3060</span> <span class="o">...</span>    <span class="n">On</span>  <span class="o">|</span> <span class="mi">00000000</span><span class="p">:</span><span class="mi">01</span><span class="p">:</span><span class="mf">00.0</span> <span class="n">Off</span> <span class="o">|</span>                  <span class="n">N</span><span class="o">/</span><span class="n">A</span> <span class="o">|</span>
<span class="o">|</span> <span class="n">N</span><span class="o">/</span><span class="n">A</span>   <span class="mi">48</span><span class="n">C</span>    <span class="n">P8</span>               <span class="mi">8</span><span class="n">W</span> <span class="o">/</span>  <span class="mi">60</span><span class="n">W</span> <span class="o">|</span>      <span class="mi">0</span><span class="n">MiB</span> <span class="o">/</span>  <span class="mi">6144</span><span class="n">MiB</span> <span class="o">|</span>      <span class="mi">0</span><span class="o">%</span>      <span class="n">Default</span> <span class="o">|</span>
<span class="o">|</span>                                         <span class="o">|</span>                      <span class="o">|</span>                  <span class="n">N</span><span class="o">/</span><span class="n">A</span> <span class="o">|</span>
<span class="o">+-----------------------------------------+----------------------+----------------------+</span>

<span class="o">+---------------------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">Processes</span><span class="p">:</span>                                                                            <span class="o">|</span>
<span class="o">|</span>  <span class="n">GPU</span>   <span class="n">GI</span>   <span class="n">CI</span>        <span class="n">PID</span>   <span class="n">Type</span>   <span class="n">Process</span> <span class="n">name</span>                            <span class="n">GPU</span> <span class="n">Memory</span> <span class="o">|</span>
<span class="o">|</span>        <span class="n">ID</span>   <span class="n">ID</span>                                                             <span class="n">Usage</span>      <span class="o">|</span>
<span class="o">|=======================================================================================|</span>
<span class="o">|</span>  <span class="n">No</span> <span class="n">running</span> <span class="n">processes</span> <span class="n">found</span>                                                           <span class="o">|</span>
<span class="o">+---------------------------------------------------------------------------------------+</span>
</pre></div>
</div>
<p>输出结果可能会因Google Colab的配置而有所不同。</p>
</div>
<div class="section" id="cola">
<h2><span class="section-number">2.5. </span>加载CoLA数据集<a class="headerlink" href="#cola" title="Permalink to this heading">¶</a></h2>
<p>使用<a class="reference external" href="(https://nyu-mll.github.io/CoLA/)">CoLA数据集</a>：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span># Hide outputs
!curl -L https://raw.githubusercontent.com/Denis2054/Transformers-for-NLP-2nd-Edition/master/Chapter03/in_domain_train.tsv --output &quot;in_domain_
train.tsv&quot;
!curl -L https://raw.githubusercontent.com/Denis2054/Transformers-for-NLP-2nd-Edition/master/Chapter03/out_of_domain_dev.tsv --output &quot;out_of_
domain_dev.tsv&quot;
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;in_domain_train.tsv&quot;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;sentence_source&quot;</span><span class="p">,</span> <span class="s2">&quot;label&quot;</span><span class="p">,</span> <span class="s2">&quot;label_notes&quot;</span><span class="p">,</span> <span class="s2">&quot;sentence&quot;</span><span class="p">])</span>
<span class="n">df</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">8551</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
<p>可以看到，数据集的形状是<code class="docutils literal notranslate"><span class="pre">(8551,</span> <span class="pre">4)</span></code>，说明我们加载了8551条数据，每条数据包含4个属性。</p>
<p>从中取10条数据看看：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sentence_source</th>
      <th>label</th>
      <th>label_notes</th>
      <th>sentence</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1281</th>
      <td>r-67</td>
      <td>1</td>
      <td>NaN</td>
      <td>i went to the store and have bought some excel...</td>
    </tr>
    <tr>
      <th>1479</th>
      <td>r-67</td>
      <td>1</td>
      <td>NaN</td>
      <td>it is not true that that it would confuse the ...</td>
    </tr>
    <tr>
      <th>3687</th>
      <td>ks08</td>
      <td>1</td>
      <td>NaN</td>
      <td>no doubt that he was forced to leave his famil...</td>
    </tr>
    <tr>
      <th>7401</th>
      <td>sks13</td>
      <td>0</td>
      <td>*</td>
      <td>they are special of bill .</td>
    </tr>
    <tr>
      <th>2772</th>
      <td>l-93</td>
      <td>1</td>
      <td>NaN</td>
      <td>steve tossed the ball into the garden .</td>
    </tr>
    <tr>
      <th>8152</th>
      <td>ad03</td>
      <td>1</td>
      <td>NaN</td>
      <td>many fish are in the sea .</td>
    </tr>
    <tr>
      <th>6304</th>
      <td>c_13</td>
      <td>1</td>
      <td>NaN</td>
      <td>calvin has dated every girl who jeff has .</td>
    </tr>
    <tr>
      <th>8187</th>
      <td>ad03</td>
      <td>1</td>
      <td>NaN</td>
      <td>alison and david soaked their feet in the kitchen</td>
    </tr>
    <tr>
      <th>4728</th>
      <td>ks08</td>
      <td>0</td>
      <td>*</td>
      <td>seoul was slept in by the businessman last nig...</td>
    </tr>
    <tr>
      <th>2624</th>
      <td>l-93</td>
      <td>1</td>
      <td>NaN</td>
      <td>the king banished the general from the army .</td>
    </tr>
  </tbody>
</table>
</div><p><code class="docutils literal notranslate"><span class="pre">.tsv</span></code>文件中的每个样本包含四列，以制表符分隔：</p>
<ul class="simple">
<li><p>第一列：句子（代码）的来源</p></li>
<li><p>第二列：标签（0=不可接受，1=可接受）</p></li>
<li><p>第三列：作者标注的标签</p></li>
<li><p>第四列：待分类的句子</p></li>
</ul>
</div>
<div class="section" id="tokens">
<h2><span class="section-number">2.6. </span>创建句子和标签列表并添加特殊Tokens<a class="headerlink" href="#tokens" title="Permalink to this heading">¶</a></h2>
<p>将所需分类的句子和分类标签从数据集表格中取出来，并在句子的开头和结尾分别加上<code class="docutils literal notranslate"><span class="pre">[CLS]</span></code>和<code class="docutils literal notranslate"><span class="pre">[SEP]</span></code>特殊token.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sentences</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sentence</span><span class="o">.</span><span class="n">values</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;[CLS] &quot;</span> <span class="o">+</span> <span class="n">sentence</span> <span class="o">+</span> <span class="s2">&quot; [SEP]&quot;</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">values</span>
</pre></div>
</div>
</div>
<div class="section" id="id3">
<h2><span class="section-number">2.7. </span>使用BERT分词器<a class="headerlink" href="#id3" title="Permalink to this heading">¶</a></h2>
<p>在本节中，我们将初始化一个预训练的BERT分词器。这样可以节省从头开始训练分词器所需的时间。</p>
<p>我们使用一个不区分大小写的分词器，然后打印第一个分词后的句子：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">,</span> <span class="n">do_lower_case</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tokenized_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">sent</span><span class="p">)</span> <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">]</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Tokenize the first sentence:&quot;</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">tokenized_texts</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Tokenize</span> <span class="n">the</span> <span class="n">first</span> <span class="n">sentence</span><span class="p">:</span>
<span class="p">[</span><span class="s1">&#39;[CLS]&#39;</span><span class="p">,</span> <span class="s1">&#39;our&#39;</span><span class="p">,</span> <span class="s1">&#39;friends&#39;</span><span class="p">,</span> <span class="s1">&#39;wo&#39;</span><span class="p">,</span> <span class="s1">&#39;n&#39;</span><span class="p">,</span> <span class="s2">&quot;&#39;&quot;</span><span class="p">,</span> <span class="s1">&#39;t&#39;</span><span class="p">,</span> <span class="s1">&#39;buy&#39;</span><span class="p">,</span> <span class="s1">&#39;this&#39;</span><span class="p">,</span> <span class="s1">&#39;analysis&#39;</span><span class="p">,</span> <span class="s1">&#39;,&#39;</span><span class="p">,</span> <span class="s1">&#39;let&#39;</span><span class="p">,</span> <span class="s1">&#39;alone&#39;</span><span class="p">,</span> <span class="s1">&#39;the&#39;</span><span class="p">,</span> <span class="s1">&#39;next&#39;</span><span class="p">,</span> <span class="s1">&#39;one&#39;</span><span class="p">,</span> <span class="s1">&#39;we&#39;</span><span class="p">,</span> <span class="s1">&#39;propose&#39;</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="s1">&#39;[SEP]&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>可以看到，分词器的输出包含了分类token <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code>和分隔token <code class="docutils literal notranslate"><span class="pre">[SEP]</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">BertTokenizer.from_pretrained('bert-base-uncased',</span> <span class="pre">do_lower_case=True)</span></code>需要从huggingface.com上下载内容，可能会因为网络原因报错，有以下解决方案：</p>
<ul class="simple">
<li><p>使用全局梯子</p></li>
<li><p>从镜像网站<code class="docutils literal notranslate"><span class="pre">https://hf-mirror.com/google-bert/bert-base-uncased/tree/main</span></code>下载以下文件并保存到运行目录下的<code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>文件夹：</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">config.json</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pytorch_model.bin</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tokenizer.json</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tokenizer_config.json</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">vocab.txt</span></code></p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="id4">
<h2><span class="section-number">2.8. </span>数据处理<a class="headerlink" href="#id4" title="Permalink to this heading">¶</a></h2>
<p>为了使数据可以成批地输入模型中，我们需要让他们拥有相同的长度。我们需要确定一个固定的最大长度。数据集中的句子很短，但为了确保这一点，程序将序列的最大长度设置为128，并对不足128的token序列进行填充：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">MAX_LEN</span> <span class="o">=</span> <span class="mi">128</span>
<span class="c1"># 使用BERT分词器将token转换为BERT词表中的索引</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tokenized_texts</span><span class="p">]</span>
<span class="c1"># 对输入tokens进行填充</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">MAX_LEN</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;long&quot;</span><span class="p">,</span> <span class="n">truncating</span><span class="o">=</span><span class="s2">&quot;post&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;post&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span>  <span class="mi">101</span>  <span class="mi">2256</span>  <span class="mi">2814</span> <span class="mi">24185</span>  <span class="mi">1050</span>  <span class="mi">1005</span>  <span class="mi">1056</span>  <span class="mi">4965</span>  <span class="mi">2023</span>  <span class="mi">4106</span>  <span class="mi">1010</span>  <span class="mi">2292</span>
  <span class="mi">2894</span>  <span class="mi">1996</span>  <span class="mi">2279</span>  <span class="mi">2028</span>  <span class="mi">2057</span> <span class="mi">16599</span>  <span class="mi">1012</span>   <span class="mi">102</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>我们打印第一个token序列，发现序列尾部有一大串0，这就是填充后的结果。在BERT模型中，填充token表示为<code class="docutils literal notranslate"><span class="pre">[PAD]</span></code>，其在词表中的索引为0.</p>
</div>
<div class="section" id="id5">
<h2><span class="section-number">2.9. </span>创建注意力遮罩<a class="headerlink" href="#id5" title="Permalink to this heading">¶</a></h2>
<p>填充的tokens唯一作用只是用来保证输入形状统一，它不应该参与注意力的计算而影响最终的结果。因此，我们要创建注意力遮罩来禁止模型对填充字符施加注意力。</p>
<p>一种简单的做法是将非填充token部分的注意力遮罩的值置为1，填充token部分的注意力遮罩的值置为0：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">attention_masks</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">input_ids</span><span class="p">:</span>
    <span class="n">seq_mask</span> <span class="o">=</span> <span class="p">[</span><span class="nb">float</span><span class="p">(</span><span class="n">i</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">seq</span><span class="p">]</span>
    <span class="n">attention_masks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">seq_mask</span><span class="p">)</span>
</pre></div>
</div>
<p>查看相互对应的token序列和注意力遮罩：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">attention_masks</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span>  <span class="mi">101</span>  <span class="mi">2028</span>  <span class="mi">2062</span> <span class="mi">18404</span>  <span class="mi">2236</span>  <span class="mi">3989</span>  <span class="mi">2030</span>  <span class="mi">1045</span>  <span class="mi">1005</span>  <span class="mi">1049</span>  <span class="mi">3228</span>  <span class="mi">2039</span>
  <span class="mi">1012</span>   <span class="mi">102</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span><span class="p">]</span>
<span class="p">[</span><span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span>
 <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span>
 <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span>
 <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span>
 <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span>
 <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span><span class="p">]</span>
</pre></div>
</div>
<p>可以看到，token序列中填充部分的注意力遮罩的值为0，非填充部分则为1</p>
</div>
<div class="section" id="id6">
<h2><span class="section-number">2.10. </span>分割训练集和验证集<a class="headerlink" href="#id6" title="Permalink to this heading">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_inputs</span><span class="p">,</span> <span class="n">validation_inputs</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">validation_labels</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2018</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">train_masks</span><span class="p">,</span> <span class="n">validation_masks</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">attention_masks</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2018</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
<p>其中<code class="docutils literal notranslate"><span class="pre">test_size=0.1</span></code>表示分割10%作为验证集，<code class="docutils literal notranslate"><span class="pre">random_state=2018</span></code>表示固定2018作为随机分割的随机种子，保证每次分割的结果是相同的。</p>
</div>
<div class="section" id="tensor">
<h2><span class="section-number">2.11. </span>把所有数据转换为tensor<a class="headerlink" href="#tensor" title="Permalink to this heading">¶</a></h2>
<p>我们所微调的BERT模型采用PyTorch张量（tensor）作为输入和输出，因此我们要将数据转化为tensor.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">train_inputs</span><span class="p">)</span>
<span class="n">validation_inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">validation_inputs</span><span class="p">)</span>
<span class="n">train_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">train_labels</span><span class="p">)</span>
<span class="n">validation_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">validation_labels</span><span class="p">)</span>
<span class="n">train_masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">train_masks</span><span class="p">)</span>
<span class="n">validation_masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">validation_masks</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="batch-size">
<h2><span class="section-number">2.12. </span>设置一个批次大小（Batch Size）并创建数据迭代器<a class="headerlink" href="#batch-size" title="Permalink to this heading">¶</a></h2>
<p>批次大小（Batch
Size）表示同时并行输入模型的数据的数量，数据迭代器可以将数据集进行封装，并以指定的batch
size和模式（随机/顺序）</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>

<span class="c1"># 将多个tensor列表封装成tensor数据集</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">train_inputs</span><span class="p">,</span> <span class="n">train_masks</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)</span>
<span class="c1"># 随机采样器</span>
<span class="n">train_sampler</span> <span class="o">=</span> <span class="n">RandomSampler</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
<span class="c1"># 创建一个DataLoader迭代器，可以以随机的顺序和`batch_size`的批次大小遍历数据集</span>
<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>

<span class="n">validation_data</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">validation_inputs</span><span class="p">,</span> <span class="n">validation_masks</span><span class="p">,</span> <span class="n">validation_labels</span><span class="p">)</span>
<span class="c1"># 顺序采样器</span>
<span class="n">validation_sampler</span> <span class="o">=</span> <span class="n">SequentialSampler</span><span class="p">(</span><span class="n">validation_data</span><span class="p">)</span>
<span class="c1"># 创建一个DataLoader迭代器，可以以原始的顺序和`batch_size`的批次大小遍历数据集</span>
<span class="n">validation_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">validation_data</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">validation_sampler</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="id7">
<h2><span class="section-number">2.13. </span>BERT模型配置<a class="headerlink" href="#id7" title="Permalink to this heading">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertConfig</span><span class="p">,</span> <span class="n">BertModel</span>

<span class="n">configuration</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="p">()</span>
<span class="c1"># 通过配置初始化BERT模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="p">(</span><span class="n">configuration</span><span class="p">)</span>
<span class="c1"># 访问模型的配置</span>
<span class="n">configuration</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span>
<span class="nb">print</span><span class="p">(</span><span class="n">configuration</span><span class="p">)</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">BertConfig</span> <span class="p">{</span>
  <span class="s2">&quot;attention_probs_dropout_prob&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
  <span class="s2">&quot;classifier_dropout&quot;</span><span class="p">:</span> <span class="n">null</span><span class="p">,</span>
  <span class="s2">&quot;hidden_act&quot;</span><span class="p">:</span> <span class="s2">&quot;gelu&quot;</span><span class="p">,</span>
  <span class="s2">&quot;hidden_dropout_prob&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
  <span class="s2">&quot;hidden_size&quot;</span><span class="p">:</span> <span class="mi">768</span><span class="p">,</span>
  <span class="s2">&quot;initializer_range&quot;</span><span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>
  <span class="s2">&quot;intermediate_size&quot;</span><span class="p">:</span> <span class="mi">3072</span><span class="p">,</span>
  <span class="s2">&quot;layer_norm_eps&quot;</span><span class="p">:</span> <span class="mf">1e-12</span><span class="p">,</span>
  <span class="s2">&quot;max_position_embeddings&quot;</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
  <span class="s2">&quot;model_type&quot;</span><span class="p">:</span> <span class="s2">&quot;bert&quot;</span><span class="p">,</span>
  <span class="s2">&quot;num_attention_heads&quot;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span>
  <span class="s2">&quot;num_hidden_layers&quot;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span>
  <span class="s2">&quot;pad_token_id&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
  <span class="s2">&quot;position_embedding_type&quot;</span><span class="p">:</span> <span class="s2">&quot;absolute&quot;</span><span class="p">,</span>
  <span class="s2">&quot;transformers_version&quot;</span><span class="p">:</span> <span class="s2">&quot;4.39.2&quot;</span><span class="p">,</span>
  <span class="s2">&quot;type_vocab_size&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
  <span class="s2">&quot;use_cache&quot;</span><span class="p">:</span> <span class="n">true</span><span class="p">,</span>
  <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">30522</span>
<span class="p">}</span>
</pre></div>
</div>
<p>我们来看看其中的一些主要配置：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">attention_probs_dropout_prob:</span> <span class="pre">0.1</span></code>
表示对注意力概率应用0.1的Dropout率（Dropout Rate）。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hidden_act:</span> <span class="pre">&quot;gelu&quot;</span></code>
是编码器中的非线性激活函数。它是高斯误差线性单元激活函数。输入通过其幅度加权，使其非线性化。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hidden_dropout_prob:</span> <span class="pre">0.1</span></code>
是应用于全连接层的Dropout概率。全连接可以在嵌入层、编码器和池化器层中找到。输出并不总是对序列内容的良好反映。对隐藏状态序列进行池化可以改善输出序列。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hidden_size:</span> <span class="pre">768</span></code>
是编码层和池化器层的维度，即<span class="math notranslate nohighlight">\(d_\text{model}\)</span>。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">initializer_range:</span> <span class="pre">0.02</span></code> 是初始化权重矩阵时的标准差值。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">intermediate_size:</span> <span class="pre">3072</span></code> 是编码器中前馈层的维度。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">layer_norm_eps:</span> <span class="pre">1e-12</span></code> 是用于层归一化层的epsilon值。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_position_embeddings:</span> <span class="pre">512</span></code> 是模型使用的最大长度。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model_type:</span> <span class="pre">&quot;bert&quot;</span></code> 是模型的名称。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_attention_heads:</span> <span class="pre">12</span></code> 是注意力头的数量。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_hidden_layers:</span> <span class="pre">12</span></code> 是编码器层的数量。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pad_token_id:</span> <span class="pre">0</span></code> 是填充标记的ID，以避免对填充标记进行训练。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">type_vocab_size:</span> <span class="pre">2</span></code>
是token_type_ids的大小，用于识别序列。例如，“the dog [SEP] The cat
[SEP]” 可以用token ID [0,0,0,1,1,1]表示。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">vocab_size:</span> <span class="pre">30522</span></code> 是模型用于表示input_ids的不同标记的数量。</p></li>
</ul>
</div>
<div class="section" id="hugging-facebert">
<h2><span class="section-number">2.14. </span>加载Hugging Face预训练BERT模型<a class="headerlink" href="#hugging-facebert" title="Permalink to this heading">¶</a></h2>
<p>在了解了BERT模型的配置后我们来加载BERT的预训练模型。</p>
<p>这里使用的是<code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>模型，base表示模型的大小，uncased表示模型的词表和训练数据是不区分大小写的。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 加载预训练模型，`num_labels=2`表示我们将要利用预训练模型来进行2分类任务</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># 对模型采用数据并行</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="c1"># 将模型发送到期望的设备（GPU）上</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Some</span> <span class="n">weights</span> <span class="n">of</span> <span class="n">BertForSequenceClassification</span> <span class="n">were</span> <span class="ow">not</span> <span class="n">initialized</span> <span class="kn">from</span> <span class="nn">the</span> <span class="n">model</span> <span class="n">checkpoint</span> <span class="n">at</span> <span class="n">bert</span><span class="o">-</span><span class="n">base</span><span class="o">-</span><span class="n">uncased</span> <span class="ow">and</span> <span class="n">are</span> <span class="n">newly</span> <span class="n">initialized</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;classifier.bias&#39;</span><span class="p">,</span> <span class="s1">&#39;classifier.weight&#39;</span><span class="p">]</span>
<span class="n">You</span> <span class="n">should</span> <span class="n">probably</span> <span class="n">TRAIN</span> <span class="n">this</span> <span class="n">model</span> <span class="n">on</span> <span class="n">a</span> <span class="n">down</span><span class="o">-</span><span class="n">stream</span> <span class="n">task</span> <span class="n">to</span> <span class="n">be</span> <span class="n">able</span> <span class="n">to</span> <span class="n">use</span> <span class="n">it</span> <span class="k">for</span> <span class="n">predictions</span> <span class="ow">and</span> <span class="n">inference</span><span class="o">.</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">DataParallel</span><span class="p">(</span>
  <span class="p">(</span><span class="n">module</span><span class="p">):</span> <span class="n">BertForSequenceClassification</span><span class="p">(</span>
    <span class="p">(</span><span class="n">bert</span><span class="p">):</span> <span class="n">BertModel</span><span class="p">(</span>
      <span class="p">(</span><span class="n">embeddings</span><span class="p">):</span> <span class="n">BertEmbeddings</span><span class="p">(</span>
        <span class="p">(</span><span class="n">word_embeddings</span><span class="p">):</span> <span class="n">Embedding</span><span class="p">(</span><span class="mi">30522</span><span class="p">,</span> <span class="mi">768</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="p">(</span><span class="n">position_embeddings</span><span class="p">):</span> <span class="n">Embedding</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">768</span><span class="p">)</span>
        <span class="p">(</span><span class="n">token_type_embeddings</span><span class="p">):</span> <span class="n">Embedding</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">768</span><span class="p">)</span>
        <span class="p">(</span><span class="n">LayerNorm</span><span class="p">):</span> <span class="n">LayerNorm</span><span class="p">((</span><span class="mi">768</span><span class="p">,),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="p">)</span>
      <span class="p">(</span><span class="n">encoder</span><span class="p">):</span> <span class="n">BertEncoder</span><span class="p">(</span>
        <span class="p">(</span><span class="n">layer</span><span class="p">):</span> <span class="n">ModuleList</span><span class="p">(</span>
          <span class="p">(</span><span class="mi">0</span><span class="o">-</span><span class="mi">11</span><span class="p">):</span> <span class="mi">12</span> <span class="n">x</span> <span class="n">BertLayer</span><span class="p">(</span>
            <span class="p">(</span><span class="n">attention</span><span class="p">):</span> <span class="n">BertAttention</span><span class="p">(</span>
              <span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="n">BertSelfAttention</span><span class="p">(</span>
                <span class="p">(</span><span class="n">query</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="p">(</span><span class="n">key</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="p">(</span><span class="n">value</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
              <span class="p">)</span>
              <span class="p">(</span><span class="n">output</span><span class="p">):</span> <span class="n">BertSelfOutput</span><span class="p">(</span>
                <span class="p">(</span><span class="n">dense</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="p">(</span><span class="n">LayerNorm</span><span class="p">):</span> <span class="n">LayerNorm</span><span class="p">((</span><span class="mi">768</span><span class="p">,),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
              <span class="p">)</span>
            <span class="p">)</span>
            <span class="p">(</span><span class="n">intermediate</span><span class="p">):</span> <span class="n">BertIntermediate</span><span class="p">(</span>
              <span class="p">(</span><span class="n">dense</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">3072</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
              <span class="p">(</span><span class="n">intermediate_act_fn</span><span class="p">):</span> <span class="n">GELUActivation</span><span class="p">()</span>
            <span class="p">)</span>
            <span class="p">(</span><span class="n">output</span><span class="p">):</span> <span class="n">BertOutput</span><span class="p">(</span>
              <span class="p">(</span><span class="n">dense</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">3072</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
              <span class="p">(</span><span class="n">LayerNorm</span><span class="p">):</span> <span class="n">LayerNorm</span><span class="p">((</span><span class="mi">768</span><span class="p">,),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
              <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="p">)</span>
          <span class="p">)</span>
        <span class="p">)</span>
      <span class="p">)</span>
      <span class="p">(</span><span class="n">pooler</span><span class="p">):</span> <span class="n">BertPooler</span><span class="p">(</span>
        <span class="p">(</span><span class="n">dense</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="p">(</span><span class="n">activation</span><span class="p">):</span> <span class="n">Tanh</span><span class="p">()</span>
      <span class="p">)</span>
    <span class="p">)</span>
    <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="p">(</span><span class="n">classifier</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>上面的代码加载了预训练模型，对模型采用数据并行，并将模型发送到GPU上。代码输出了模型的结构，可以看到BERT分类模型<code class="docutils literal notranslate"><span class="pre">BertForSequenceClassification</span></code>中包含了BERT骨干模型（Backbone
Model）<code class="docutils literal notranslate"><span class="pre">BertModel</span></code>，<code class="docutils literal notranslate"><span class="pre">BertModel</span></code>中又包含了嵌入层<code class="docutils literal notranslate"><span class="pre">BertEmbeddings</span></code>和编码器<code class="docutils literal notranslate"><span class="pre">BertEncoder</span></code>，编码器<code class="docutils literal notranslate"><span class="pre">BertEncoder</span></code>中又包含一个<code class="docutils literal notranslate"><span class="pre">ModuleList</span></code>，其中有12个<code class="docutils literal notranslate"><span class="pre">BertLayer</span></code>.
有兴趣可以阅读<a class="reference external" href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py#L1494">BertForSequenceClassification的源码</a>。</p>
<p><code class="docutils literal notranslate"><span class="pre">BertForSequenceClassification.from_pretrained(&quot;bert-base-uncased&quot;,</span> <span class="pre">num_labels=2)</span></code>也需要从huggingface.com上下载内容，可能会因为网络原因报错，解决方法同上。</p>
</div>
<div class="section" id="id8">
<h2><span class="section-number">2.15. </span>将模型参数分组<a class="headerlink" href="#id8" title="Permalink to this heading">¶</a></h2>
<p>通过<code class="docutils literal notranslate"><span class="pre">model.named_paramters()</span></code>方法可以获取模型参数的名字和参数的值：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">param_optimizer</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">())</span>
<span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">param_optimizer</span><span class="p">]</span>
<span class="n">values</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">param_optimizer</span><span class="p">]</span>
</pre></div>
</div>
<p>为了防止过拟合，一种常见的方法是应用<a class="reference external" href="https://zhuanlan.zhihu.com/p/29360425">正则化</a>。pytorch中的AdamW优化器可以通过指定<code class="docutils literal notranslate"><span class="pre">weight_decay</span></code>参数来控制正则化的力度。默认情况下<code class="docutils literal notranslate"><span class="pre">weight_decay=0.0</span></code>，即不进行正则化.</p>
<p>然而，在BERT模型中，并不是所有参数都需要进行正则化，因此我们需要将模型的参数分成两组，一组<code class="docutils literal notranslate"><span class="pre">weight_decay=0.1</span></code>，另一组<code class="docutils literal notranslate"><span class="pre">weight_decay=0</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">param_optimizer</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">())</span>
<span class="n">no_decay</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="s1">&#39;LayerNorm.weight&#39;</span><span class="p">]</span>
<span class="n">optimizer_grouped_parameters</span> <span class="o">=</span> <span class="p">[</span>
    <span class="c1"># 参数名称不包含`bias`和`LayerNorm.weight`的参数：正则化力度为0.1</span>
    <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">param_optimizer</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span><span class="n">nd</span> <span class="ow">in</span> <span class="n">n</span> <span class="k">for</span> <span class="n">nd</span> <span class="ow">in</span> <span class="n">no_decay</span><span class="p">)],</span>
     <span class="s1">&#39;weight_decay_rate&#39;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">},</span>
    <span class="c1"># 参数名称包含`bias`和`LayerNorm.weight`的参数：正则化力度为0.0</span>
    <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">param_optimizer</span> <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">nd</span> <span class="ow">in</span> <span class="n">n</span> <span class="k">for</span> <span class="n">nd</span> <span class="ow">in</span> <span class="n">no_decay</span><span class="p">)],</span>
     <span class="s1">&#39;weight_decay_rate&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">}</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
<div class="section" id="id9">
<h2><span class="section-number">2.16. </span>训练流程<a class="headerlink" href="#id9" title="Permalink to this heading">¶</a></h2>
<p>训练循环遵循标准的学习过程。将迭代的次数设置为4，并将损失和准确率的度量结果绘制出来。训练循环使用数据加载器来加载和训练批次数据。训练过程进行度量和评估。</p>
<p>BERT的训练流程遵循标准的深度学习的训练流程。我们将迭代次数（Epochs）设为4，并在训练过程中将模型的损失和准确率绘制出来。</p>
<p>首先创建优化器和学习率调度器，并指定模型训练的相关超参数：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 训练轮数</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">4</span>

<span class="c1"># 创建优化器</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span>
    <span class="n">optimizer_grouped_parameters</span><span class="p">,</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="mf">2e-5</span><span class="p">,</span> <span class="c1"># 学习率</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-8</span> <span class="c1"># AdamW的epsilon参数，一般是一个接近0的值，防止优化目标计算时分母为0</span>
<span class="p">)</span>

<span class="c1"># 总的训练步数 = 数据加载器长度 * 训练轮数</span>
<span class="n">total_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span> <span class="o">*</span> <span class="n">epochs</span>

<span class="c1"># 创建学习率调度器</span>
<span class="c1"># 这里表示创建一个线性的学习率调度器，</span>
<span class="c1"># 不进行warmup，在`total_steps`步中将学习率从上述`2e-5`降到`0`</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">get_linear_schedule_with_warmup</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">num_warmup_steps</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">num_training_steps</span> <span class="o">=</span> <span class="n">total_steps</span>
<span class="p">)</span>
</pre></div>
</div>
<p>然后创建准确率衡量函数：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 创建准确率衡量函数，计算预测值和标签的准确率</span>
<span class="k">def</span> <span class="nf">flat_accuracy</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="n">pred_flat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">labels_flat</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">pred_flat</span> <span class="o">==</span> <span class="n">labels_flat</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels_flat</span><span class="p">)</span>
</pre></div>
</div>
<p>最后正式开始训练流程：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">4</span>

<span class="c1"># 用于存储我们的损失和准确率，以便绘图</span>
<span class="n">train_loss_set</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># trange是对普通的Python range函数的tqdm包装</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">trange</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Epoch&quot;</span><span class="p">):</span>

    <span class="c1"># 训练</span>

    <span class="c1"># 将模型设置为训练模式（与评估模式相对）</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

    <span class="c1"># 追踪变量</span>
    <span class="n">tr_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">nb_tr_examples</span><span class="p">,</span> <span class="n">nb_tr_steps</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>

    <span class="c1"># 遍历训练集的DataLoader，完成1个epoch的训练</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
        <span class="c1"># 将一个batch的数据移到GPU上</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">)</span>
        <span class="c1"># 从DataLoader中unpack输入</span>
        <span class="n">b_input_ids</span><span class="p">,</span> <span class="n">b_input_mask</span><span class="p">,</span> <span class="n">b_labels</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="c1"># 清除梯度（因为默认情况下pytorch的优化器会对梯度进行累积）</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="c1"># 模型前向传播</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span>
            <span class="n">b_input_ids</span><span class="p">,</span>
            <span class="n">token_type_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">b_input_mask</span><span class="p">,</span>
            <span class="n">labels</span><span class="o">=</span><span class="n">b_labels</span>
        <span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span>
        <span class="n">train_loss_set</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="c1"># 反向传播</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="c1"># 更新参数并根据计算的梯度进行一步优化</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="c1"># 更新追踪变量</span>
        <span class="n">tr_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">nb_tr_examples</span> <span class="o">+=</span> <span class="n">b_input_ids</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">nb_tr_steps</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train loss: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tr_loss</span> <span class="o">/</span> <span class="n">nb_tr_steps</span><span class="p">))</span>

    <span class="c1"># 验证</span>

    <span class="c1"># 将模型设置为评估模式，以评估验证集上的损失</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

    <span class="c1"># 追踪变量</span>
    <span class="n">eval_loss</span><span class="p">,</span> <span class="n">eval_accuracy</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="n">nb_eval_steps</span><span class="p">,</span> <span class="n">nb_eval_examples</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>

    <span class="c1"># 在一个epoch内评估数据</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">validation_dataloader</span><span class="p">:</span>
        <span class="c1"># 将批次移到GPU上</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">)</span>
        <span class="c1"># 从数据加载器中解包输入</span>
        <span class="n">b_input_ids</span><span class="p">,</span> <span class="n">b_input_mask</span><span class="p">,</span> <span class="n">b_labels</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="c1"># 告诉模型不要计算或存储梯度，以节省内存并加快验证速度</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="c1"># 前向传播，计算logit预测</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">b_input_ids</span><span class="p">,</span>
                           <span class="n">token_type_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">attention_mask</span><span class="o">=</span><span class="n">b_input_mask</span><span class="p">)</span>

        <span class="c1"># 将logits和标签移到CPU上</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="s1">&#39;logits&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">label_ids</span> <span class="o">=</span> <span class="n">b_labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="n">tmp_eval_accuracy</span> <span class="o">=</span> <span class="n">flat_accuracy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">label_ids</span><span class="p">)</span>

        <span class="n">eval_accuracy</span> <span class="o">+=</span> <span class="n">tmp_eval_accuracy</span>
        <span class="n">nb_eval_steps</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Validation Accuracy: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">eval_accuracy</span> <span class="o">/</span> <span class="n">nb_eval_steps</span><span class="p">))</span>
</pre></div>
</div>
<p>执行上述代码后，BERT模型将会在CoLA数据集上进行二分类任务的微调。每轮训练的损失和分类预测准确率会被打印出来。</p>
</div>
<div class="section" id="id10">
<h2><span class="section-number">2.17. </span>训练评估<a class="headerlink" href="#id10" title="Permalink to this heading">¶</a></h2>
<p>训练过程中模型的损失存储在<code class="docutils literal notranslate"><span class="pre">train_loss_set</span></code>中，我们可以将损失随训练过程的变化进行可视化：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Training loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Batch&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_loss_set</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>上述代码执行后可以看到，随着训练的进行，模型的损失呈波动下降趋势。</p>
</div>
<div class="section" id="id11">
<h2><span class="section-number">2.18. </span>使用测试集进行预测和评估<a class="headerlink" href="#id11" title="Permalink to this heading">¶</a></h2>
<p>BERT下游模型是使用<code class="docutils literal notranslate"><span class="pre">in_domain_train.tsv</span></code>数据集进行训练的。现在，我们将使用另一个数据集，<code class="docutils literal notranslate"><span class="pre">out_of_domain_dev.tsv</span></code>数据集来预测句子语法是否正确。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;out_of_domain_dev.tsv&quot;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;sentence_source&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">,</span> <span class="s1">&#39;label_notes&#39;</span><span class="p">,</span> <span class="s1">&#39;sentence&#39;</span><span class="p">])</span>
<span class="n">df</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">516</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 创建句子和标签的列表</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sentence</span><span class="o">.</span><span class="n">values</span>

<span class="c1"># 添加特殊token</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;[CLS] &quot;</span> <span class="o">+</span> <span class="n">sentence</span> <span class="o">+</span> <span class="s2">&quot; [SEP]&quot;</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">values</span>

<span class="n">tokenized_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">sent</span><span class="p">)</span> <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">]</span>


<span class="n">MAX_LEN</span> <span class="o">=</span> <span class="mi">128</span>

<span class="c1"># 使用BERT分词器将token序列转换为其在词表中的索引</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tokenized_texts</span><span class="p">]</span>
<span class="c1"># 填充输入序列</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">MAX_LEN</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;long&quot;</span><span class="p">,</span> <span class="n">truncating</span><span class="o">=</span><span class="s2">&quot;post&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;post&quot;</span><span class="p">)</span>
<span class="c1"># 创建注意力遮罩</span>
<span class="n">attention_masks</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># 非填充部分为1，填充部分为0</span>
<span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">input_ids</span><span class="p">:</span>
    <span class="n">seq_mask</span> <span class="o">=</span> <span class="p">[</span><span class="nb">float</span><span class="p">(</span><span class="n">i</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">seq</span><span class="p">]</span>
    <span class="n">attention_masks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">seq_mask</span><span class="p">)</span>

<span class="n">prediction_inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
<span class="n">prediction_masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">attention_masks</span><span class="p">)</span>
<span class="n">prediction_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>


<span class="n">prediction_data</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">prediction_inputs</span><span class="p">,</span> <span class="n">prediction_masks</span><span class="p">,</span> <span class="n">prediction_labels</span><span class="p">)</span>
<span class="n">prediction_sampler</span> <span class="o">=</span> <span class="n">SequentialSampler</span><span class="p">(</span><span class="n">prediction_data</span><span class="p">)</span>
<span class="n">prediction_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">prediction_data</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">prediction_sampler</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 计算softmax logits</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">):</span>
    <span class="n">e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">e</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># 启用模型的评估模式</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># 追踪变量</span>
<span class="n">raw_predictions</span><span class="p">,</span> <span class="n">predicted_classes</span><span class="p">,</span> <span class="n">true_labels</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="c1"># 预测</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">prediction_dataloader</span><span class="p">:</span>
    <span class="c1"># 把batch放到GPU上</span>
    <span class="n">batch</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">)</span>
    <span class="c1"># 从dataloader中解包出数据</span>
    <span class="n">b_input_ids</span><span class="p">,</span> <span class="n">b_input_mask</span><span class="p">,</span> <span class="n">b_labels</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="c1"># 告诉模型不要保存梯度，以节约显存，提高预测速度</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="c1"># Forward pass, calculate logit predictions</span>
        <span class="c1"># 前向传播，计算logits</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">b_input_ids</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">b_input_mask</span><span class="p">)</span>

    <span class="c1"># 把logits和标签放到CPU上</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;logits&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">label_ids</span> <span class="o">=</span> <span class="n">b_labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="c1"># 把token序列转换回单词字符串</span>
    <span class="n">b_input_ids</span> <span class="o">=</span> <span class="n">b_input_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">batch_sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">input_ids</span> <span class="ow">in</span> <span class="n">b_input_ids</span><span class="p">]</span>

    <span class="c1"># 应用softmax函数，将模型输出的logits转化为概率</span>
    <span class="n">probabilities</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">logits</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># 取概率最高的类别作为预测结果</span>
    <span class="n">batch_predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">probabilities</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># 打印句子和预测结果</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batch_sentences</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sentence: </span><span class="si">{</span><span class="n">sentence</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prediction: </span><span class="si">{</span><span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sofmax probabilities&quot;</span><span class="p">,</span> <span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prediction: </span><span class="si">{</span><span class="n">batch_predictions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True label: </span><span class="si">{</span><span class="n">label_ids</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># 保存logits，预测类别和真实标签</span>
    <span class="n">raw_predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">predicted_classes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch_predictions</span><span class="p">)</span>
    <span class="n">true_labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">label_ids</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="mcc">
<h2><span class="section-number">2.19. </span>使用马修斯相关系数（MCC）进行评估<a class="headerlink" href="#mcc" title="Permalink to this heading">¶</a></h2>
<p>马修斯相关系数（MCC）的公式如下：</p>
<div class="math notranslate nohighlight" id="equation-chapter-03-fine-tuning-bert-models-section-02-fine-tuning-bert-0">
<span class="eqno">(2.19.1)<a class="headerlink" href="#equation-chapter-03-fine-tuning-bert-models-section-02-fine-tuning-bert-0" title="Permalink to this equation">¶</a></span>\[MCC = \frac{TP\times TN - FP \times FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(TP\)</span>表示True Positive（真正），即预测为正，实际标签为正</p></li>
<li><p><span class="math notranslate nohighlight">\(TN\)</span>表示True Negative（真负），即预测为正，实际标签为正</p></li>
<li><p><span class="math notranslate nohighlight">\(FP\)</span>表示False Positive（假正），即预测为正，实际标签为负</p></li>
<li><p><span class="math notranslate nohighlight">\(FN\)</span>表示False Negative（假负），即预测为负，实际标签为正</p></li>
</ul>
<p>最终的MCC得分将基于整个测试集，但让我们先看看各个batch的得分，以了解批次之间度量指标的变化情况。各batch的MCC评估代码如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">matthews_corrcoef</span>

<span class="c1"># 初始化一个空列表来存储每个batch的MCC</span>
<span class="n">matthews_set</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># 遍历batch</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">true_labels</span><span class="p">)):</span>
    <span class="c1"># 计算该batch的MCC</span>

    <span class="c1"># true_labels[i]是该batch中所有的真实标签</span>
    <span class="c1"># predicted_classes[i]是该batch中所有的预测分类</span>
    <span class="c1"># 我们不需要使用np.argmax，因为我们在前面已经将预测分类保存在了predicted_classes中</span>

    <span class="n">matthews</span> <span class="o">=</span> <span class="n">matthews_corrcoef</span><span class="p">(</span><span class="n">true_labels</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">predicted_classes</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

    <span class="c1"># 把MCC结果加到列表中</span>
    <span class="n">matthews_set</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">matthews</span><span class="p">)</span>

<span class="c1"># Now matthews_set contains the Matthews correlation coefficient for each batch</span>
<span class="c1"># 现在，matthews_set包含了每个batch的MCC</span>
</pre></div>
</div>
<p>整个测试集上的MCC评估代码如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">matthews_corrcoef</span>

<span class="c1"># true_labels和predicted_classes中每个数据对应一个batch的数据</span>
<span class="c1"># 现在我们将其展平，使列表中每个元素对应测试集的一条数据</span>
<span class="n">true_labels_flattened</span> <span class="o">=</span> <span class="p">[</span><span class="n">label</span> <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">true_labels</span> <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
<span class="n">predicted_classes_flattened</span> <span class="o">=</span> <span class="p">[</span><span class="n">pred</span> <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">predicted_classes</span> <span class="k">for</span> <span class="n">pred</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>

<span class="c1"># 为整个测试集的预测结果计算MCC分数</span>
<span class="n">mcc</span> <span class="o">=</span> <span class="n">matthews_corrcoef</span><span class="p">(</span><span class="n">true_labels_flattened</span><span class="p">,</span> <span class="n">predicted_classes_flattened</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MCC: </span><span class="si">{</span><span class="n">mcc</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="id12">
<h2><span class="section-number">2.20. </span>保存模型<a class="headerlink" href="#id12" title="Permalink to this heading">¶</a></h2>
<p>现在我们的模型一直存储在显存和内存中，程序结束时会丢失。为了能在以后也能使用该模型，需要将模型保存下来。</p>
<p>如果我们使用了<code class="docutils literal notranslate"><span class="pre">DataParallel</span></code>将模型封装为多GPU训练对象，原始的模型会被封装到<code class="docutils literal notranslate"><span class="pre">DataParallel</span></code>对象中，称为该对象的一个属性。</p>
<p>因此，我们在保存模型时需要区分模型是否经过<code class="docutils literal notranslate"><span class="pre">DataParallel</span></code>封装：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>!mkdir saved_models/
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Specify a directory to save your model and tokenizer</span>
<span class="n">save_directory</span> <span class="o">=</span> <span class="s2">&quot;saved_models/&quot;</span>


<span class="c1"># If your model is wrapped in DataParallel, access the original model using .module and then save</span>
<span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_directory</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_directory</span><span class="p">)</span>

<span class="c1"># Save the tokenizer</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_directory</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">2. 微调BERT</a><ul>
<li><a class="reference internal" href="#id1">2.1. 硬件限制</a></li>
<li><a class="reference internal" href="#pytorchhugging-face-transformer">2.2. 安装PyTorch和Hugging Face Transformer</a></li>
<li><a class="reference internal" href="#id2">2.3. 导入模块</a></li>
<li><a class="reference internal" href="#cuda">2.4. 指定CUDA作为设备</a></li>
<li><a class="reference internal" href="#cola">2.5. 加载CoLA数据集</a></li>
<li><a class="reference internal" href="#tokens">2.6. 创建句子和标签列表并添加特殊Tokens</a></li>
<li><a class="reference internal" href="#id3">2.7. 使用BERT分词器</a></li>
<li><a class="reference internal" href="#id4">2.8. 数据处理</a></li>
<li><a class="reference internal" href="#id5">2.9. 创建注意力遮罩</a></li>
<li><a class="reference internal" href="#id6">2.10. 分割训练集和验证集</a></li>
<li><a class="reference internal" href="#tensor">2.11. 把所有数据转换为tensor</a></li>
<li><a class="reference internal" href="#batch-size">2.12. 设置一个批次大小（Batch Size）并创建数据迭代器</a></li>
<li><a class="reference internal" href="#id7">2.13. BERT模型配置</a></li>
<li><a class="reference internal" href="#hugging-facebert">2.14. 加载Hugging Face预训练BERT模型</a></li>
<li><a class="reference internal" href="#id8">2.15. 将模型参数分组</a></li>
<li><a class="reference internal" href="#id9">2.16. 训练流程</a></li>
<li><a class="reference internal" href="#id10">2.17. 训练评估</a></li>
<li><a class="reference internal" href="#id11">2.18. 使用测试集进行预测和评估</a></li>
<li><a class="reference internal" href="#mcc">2.19. 使用马修斯相关系数（MCC）进行评估</a></li>
<li><a class="reference internal" href="#id12">2.20. 保存模型</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="section_01_the_architecture_of_bert.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>1. BERT模型的架构</div>
         </div>
     </a>
     <a id="button-next" href="section_03_exercise.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>3. 练习</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>