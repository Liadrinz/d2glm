<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>1.3. 我们需要什么资源 &#8212; Transformer for NLP 0.0.1 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/d2l.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2. 从Transformer的架构开始" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/index.html" />
    <link rel="prev" title="1.2. 使用Transformer优化NLP模型" href="section_02_optimizing_nlp_models_with_transformers.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="index.html"><span class="section-number">1. </span>什么是Transformer?</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active"><span class="section-number">1.3. </span>我们需要什么资源</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_01_what_are_transformers/section_03_what_resources_should_we_use.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <span class="title-text">
                  Transformer for NLP
              </span>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">1. 什么是Transformer?</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="section_01_the_ecosystem_of_transformers.html">1.1. Transformer的生态系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_02_optimizing_nlp_models_with_transformers.html">1.2. 使用Transformer优化NLP模型</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">1.3. 我们需要什么资源</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/index.html">2. 从Transformer的架构开始</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_01_the_rise_of_the_transformer_attention_is_all_you_need.html">2.1. Transformer模型架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_02_training_and_performance.html">2.2. 模型训练和表现</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_03_transformer_models_in_hugging_face.html">2.3. Hugging Face上的Transformer模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_04_exercise.html">2.4. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/index.html">3. 微调BERT模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/section_01_the_architecture_of_bert.html">3.1. BERT模型的架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/section_02_fine_tuning_bert.html">3.2. 微调BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/section_03_exercise.html">3.3. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_04_pretraining_a_roberta_model_from_scratch/index.html">4. 从头开始预训练RoBERTa模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04_pretraining_a_roberta_model_from_scratch/section_01_pretraining_from_scratch.html">4.1. 从头开始预训练</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04_pretraining_a_roberta_model_from_scratch/section_02_exercise.html">4.2. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/index.html">5. 使用Transformer进行下游NLP任务</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/section_01_transformer_performances_versus_human_baselines.html">5.1. Transformer的性能 VS 人类基准</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/section_02_running_downstream_tasks.html">5.2. 运行下游任务</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/section_03_exercise.html">5.3. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_06_machine_translation_with_the_transformer/index.html">6. 基于Transformer的机器翻译</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06_machine_translation_with_the_transformer/section_01_defining_machine_translation.html">6.1. 机器翻译的定义</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06_machine_translation_with_the_transformer/section_02_preprocessing_a_wmt_dataset.html">6.2. 预处理WMT数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06_machine_translation_with_the_transformer/section_03_evaluating_machine_translation_with_bleu.html">6.3. 使用BLEU评估机器翻译的质量</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06_machine_translation_with_the_transformer/section_04_translations_with_trax.html">6.4. 使用Trax进行翻译</a></li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <span class="title-text">
                  Transformer for NLP
              </span>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">1. 什么是Transformer?</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="section_01_the_ecosystem_of_transformers.html">1.1. Transformer的生态系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_02_optimizing_nlp_models_with_transformers.html">1.2. 使用Transformer优化NLP模型</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">1.3. 我们需要什么资源</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/index.html">2. 从Transformer的架构开始</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_01_the_rise_of_the_transformer_attention_is_all_you_need.html">2.1. Transformer模型架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_02_training_and_performance.html">2.2. 模型训练和表现</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_03_transformer_models_in_hugging_face.html">2.3. Hugging Face上的Transformer模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_04_exercise.html">2.4. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/index.html">3. 微调BERT模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/section_01_the_architecture_of_bert.html">3.1. BERT模型的架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/section_02_fine_tuning_bert.html">3.2. 微调BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/section_03_exercise.html">3.3. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_04_pretraining_a_roberta_model_from_scratch/index.html">4. 从头开始预训练RoBERTa模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04_pretraining_a_roberta_model_from_scratch/section_01_pretraining_from_scratch.html">4.1. 从头开始预训练</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04_pretraining_a_roberta_model_from_scratch/section_02_exercise.html">4.2. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/index.html">5. 使用Transformer进行下游NLP任务</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/section_01_transformer_performances_versus_human_baselines.html">5.1. Transformer的性能 VS 人类基准</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/section_02_running_downstream_tasks.html">5.2. 运行下游任务</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/section_03_exercise.html">5.3. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_06_machine_translation_with_the_transformer/index.html">6. 基于Transformer的机器翻译</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06_machine_translation_with_the_transformer/section_01_defining_machine_translation.html">6.1. 机器翻译的定义</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06_machine_translation_with_the_transformer/section_02_preprocessing_a_wmt_dataset.html">6.2. 预处理WMT数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06_machine_translation_with_the_transformer/section_03_evaluating_machine_translation_with_bleu.html">6.3. 使用BLEU评估机器翻译的质量</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06_machine_translation_with_the_transformer/section_04_translations_with_trax.html">6.4. 使用Trax进行翻译</a></li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <div class="section" id="id1">
<h1><span class="section-number">1.3. </span>我们需要什么资源<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h1>
<p>工业4.0的人工智能模糊了云平台、框架、库、编程语言和模型之间的界限。Transformer是一种新兴的模型，其生态系统的范围和数量令人惊叹。谷歌云提供了现成的Transformer模型供使用。OpenAI部署了一个几乎不需要编程既可以使用的Transformer
API。Hugging Face提供了一个云端库服务。类似的服务还有很多。</p>
<p>本节将对我们在本书中将要实现的一些Transformer生态系统进行高层次的分析。</p>
<p>在实现用于自然语言处理的Transformer时，资源的选择非常关键，关乎项目的生存。在面对不同的项目和客户时，可能需要选用不同的资源。如果你只专注于自己喜欢的解决方案，很可能在某个时刻会遭受质疑。你应该专注于你所需要的系统，而不是你喜欢的系统。本书的目标不是解释市场上存在的每种Transformer解决方案，而是旨在解释足够多的Transformer生态系统，让你能够灵活应对在NLP项目中遇到的任何情况。</p>
<p>在本节中，我们将讨论在资源的选择和使用中面临的一些挑战。但首先，让我们从API开始。</p>
<div class="section" id="transformer-api">
<h2><span class="section-number">1.3.1. </span>Transformer API<a class="headerlink" href="#transformer-api" title="Permalink to this heading">¶</a></h2>
<p>我们现在已经进入了人工智能的工业化时代。微软、谷歌、亚马逊网络服务（AWS）和IBM等公司，提供了无法超越的人工智能服务，任何开发者或开发团队都难以望其项背。科技巨头拥有价值百万美元的超级计算机，以及海量数据集用于训练Transformer模型和其他人工智能模型。</p>
<p>大型科技巨头拥有广泛的企业客户群，这些客户已经在使用它们的云服务。因此，将Transformer
API添加到现有的云架构中所需的工作量比其他解决方案要少。</p>
<p>小公司甚至个人可以通过API以几乎没有开发投入的方式访问最强大的Transformer模型。一个实习生可以在几天内实现这个API。对于这样一个简单的实现，不需要成为工程师或拥有博士学位。</p>
<p>例如，OpenAI平台现在提供了一种基于SaaS（软件即服务）的API，用于市场上一些最有效的Transformer模型。OpenAI的API购买和使用方法请参考<a class="reference external" href="https://openai.com/blog/openai-api">这里</a></p>
<p>使用OpenAI的API只需要以下简单的步骤：</p>
<ol class="arabic simple">
<li><p>购买并获取API密钥</p></li>
<li><p>使用一行代码在笔记本中导入OpenAI</p></li>
<li><p>输入元语言提示词指定你所希望完成的NLP任务</p></li>
<li><p>收到以自然语言描述的任务完成结果</p></li>
</ol>
<p>AllenNLP提供了一些机器学习的<a class="reference external" href="https://allenai.org/demos">演示程序（Demo）</a>供用户学习。同时AllenNLP可以作为软件包安装到Python环境中进行调用。例如，假设我们需要进行一个共指消解（Coreference
Resolution）任务。共指消解任务是指找到一个词所指的实体。下面展示了使用AllenNLP进行共指消解的代码及其运行结果：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span># 安装allennlp和相关软件包
!pip install -U allennlp allennlp-models spacy
# 安装可视化工具
!git clone https://github.com/sattree/gpr_pub.git
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Hide outputs</span>
<span class="kn">from</span> <span class="nn">allennlp.predictors</span> <span class="kn">import</span> <span class="n">Predictor</span>
<span class="kn">from</span> <span class="nn">gpr_pub</span> <span class="kn">import</span> <span class="n">visualization</span>
<span class="kn">from</span> <span class="nn">IPython.core.display</span> <span class="kn">import</span> <span class="n">HTML</span><span class="p">,</span> <span class="n">display</span>

<span class="c1"># 加载样式文件</span>
<span class="n">display</span><span class="p">(</span><span class="n">HTML</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s1">&#39;gpr_pub/visualization/highlight.css&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()))</span>
<span class="n">display</span><span class="p">(</span><span class="n">HTML</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s1">&#39;gpr_pub/visualization/highlight.js&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()))</span>

<span class="c1"># 加载共指消解模型</span>
<span class="n">predictor</span> <span class="o">=</span> <span class="n">Predictor</span><span class="o">.</span><span class="n">from_path</span><span class="p">(</span><span class="s2">&quot;https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 示例输入文本</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;Barack Obama was born in Hawaii. He was the 44th President of the United States.&quot;</span>

<span class="c1"># 进行共指消解预测</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">document</span><span class="o">=</span><span class="n">text</span><span class="p">)</span>

<span class="c1"># 可视化共指消解的结果</span>
<span class="n">visualization</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">allen</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">jupyter</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<div style="padding: 16px;"><span key=0 class="highlight blue" depth=0 id=0 onmouseover="handleHighlightMouseOver(this)"                 onmouseout="handleHighlightMouseOut(this)" labelPosition="left">                <span class="highlight__label"><strong>0</strong></span>                <span class="highlight__content"><span>Barack </span> <span>Obama </span></span></span><span>was </span><span>born </span><span>in </span><span>Hawaii </span><span>. </span><span key=6 class="highlight blue" depth=0 id=0 onmouseover="handleHighlightMouseOver(this)"                 onmouseout="handleHighlightMouseOut(this)" labelPosition="left">                <span class="highlight__label"><strong>0</strong></span>                <span class="highlight__content"><span>He </span></span></span><span>was </span><span>the </span><span>44th </span><span>President </span><span>of </span><span>the </span><span>United </span><span>States </span><span>. </span></div><p>可以看到，这里的共指消解为单词“He”找到了其所指的实体“Barack Obama”。</p>
</div>
<div class="section" id="api">
<h2><span class="section-number">1.3.2. </span>基于API的软件库<a class="headerlink" href="#api" title="Permalink to this heading">¶</a></h2>
<p>在这本书中，我们将探索几个库。例如，谷歌拥有世界上一些最先进的人工智能实验室。谷歌的Trax库可以在Google
Colab中通过几行代码进行安装。您可以选择免费或付费的服务。我们可以获取源代码，调整模型，甚至在自己的服务器或谷歌云上进行训练。例如，从使用现成的API定制一个用于翻译任务的Transformer模型是一个进一步的步骤。</p>
<p>然而，在某些情况下，这可能会证明既具有教育意义又有效果。我们将探索谷歌在翻译方面的最新进展，并在“<a class="reference internal" href="../chapter_06_machine_translation_with_the_transformer/index.html#chapter-6"><span class="std std-ref">基于Transformer的机器翻译</span></a>”中实现谷歌Trax。</p>
<p>在众多算法中，使用了Transformer的最著名的在线应用之一是谷歌翻译（Google
Translate）。谷歌翻译可以通过在线平台或API进行使用。</p>
<p>我们看一个使用谷歌翻译API将英文翻译成中文的例子：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>!pip install googletrans==3.1.0a0
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">googletrans</span> <span class="kn">import</span> <span class="n">Translator</span>

<span class="n">translator</span> <span class="o">=</span> <span class="n">Translator</span><span class="p">()</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">translator</span><span class="o">.</span><span class="n">translate</span><span class="p">(</span><span class="s2">&quot;Beijing is the capital of China.&quot;</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;zh-CN&quot;</span><span class="p">)</span>
<span class="n">result</span><span class="o">.</span><span class="n">text</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="s1">&#39;北京是中国的首都。&#39;</span>
</pre></div>
</div>
</div>
<div class="section" id="transformer">
<h2><span class="section-number">1.3.3. </span>预训练Transformer模型<a class="headerlink" href="#transformer" title="Permalink to this heading">¶</a></h2>
<p>大型科技公司主导着NLP市场。仅谷歌、Facebook和微软每天就运行数十亿次NLP例程，增强了它们的AI模型无与伦比的能力。这些巨头现在提供了各种各样的Transformer模型，并拥有排名靠前的基座模型。</p>
<p>然而，小型公司也看到了庞大的NLP市场，并加入了竞争。Hugging
Face现在也提供免费或付费的服务方式。对于Hugging
Face来说，要达到通过谷歌研究实验室投入数十亿美元和微软对OpenAI的资金支持所获得的效率水平将是具有挑战性的。基础模型的入门点是在超级计算机上进行完全训练的Transformer，例如GPT-3或Google
BERT。</p>
<p>Hugging
Face采用了不同的方法，并为各种任务提供了广泛且数量众多的Transformer模型，这是一种有趣的理念。Hugging
Face提供了灵活的模型选择。此外，Hugging
Face还提供了高级API和开发者可控的API。在本书的多个章节中，我们将探索Hugging
Face作为一种教育工具和特定任务的可能解决方案。</p>
<p>我们看一下利用Hugging
Face提供的软件库和预训练Transformer模型实现自然语言生成任务的代码：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span># 安装Hugging Face transformer库
!pip install transformers
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Hide outputs</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span><span class="p">,</span> <span class="n">set_seed</span>

<span class="c1"># 固定随机种子</span>
<span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># 初始化基于gpt2模型的生成器</span>
<span class="n">gpt2_generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>

<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;Hello, I&#39;m a language model.&quot;</span>

<span class="c1"># 在text后生成最多10个标记</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">gpt2_generator</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;generated_text&quot;</span><span class="p">]</span>
</pre></div>
</div>
<div class="output highlight-default notranslate"><div class="highlight"><pre><span></span><span class="s2">&quot;Hello, I&#39;m a language model. The data we could work with isn&#39;t really what you might expect. (Or, I could write&quot;</span>
</pre></div>
</div>
<p>然而，OpenAI专注于全球最强大的少数几个Transformer引擎，并可以在许多NLP任务上达到人类水平。在“<a class="reference internal" href="../chapter_07_the_rise_of_suprahuman_transformers_with_gpt_3_engines/index.html#chapter-7"><span class="std std-ref">超人类Transformer的崛起：GPT-3引擎</span></a>”中，我们将展示OpenAI的GPT-3引擎的强大能力。</p>
<p>至此，对于Transformer的资源，我们有多种选择（Transformer
API、基于API的软件库、以及预训练Transformer模型）。这些相对立且经常冲突的选择给我们留下了许多可能的实施方式。因此，我们必须明确工业4.0人工智能专家的角色。</p>
</div>
<div class="section" id="id2">
<h2><span class="section-number">1.3.4. </span>工业4.0时代人工智能专家的角色<a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h2>
<p>工业4.0将一切与一切连接在一起，无所不在。机器直接与其他机器进行通信。由人工智能驱动的物联网信号触发自动化决策，无需人为干预。自然语言处理算法发送自动报告、摘要、电子邮件、广告等等。</p>
<p>人工智能专家将需要适应这个新时代下日益自动化的任务，包括Transformer模型的实现。人工智能专家将有新的职能。如果我们按照从高到低的顺序列出一个人工智能专家需要完成的Transformer自然语言处理任务，似乎有一些高级任务对于人工智能专家来说需要很少或几乎不需要开发。一个人工智能专家可以成为在人工智能领域提供设计理念、解释和实施的大师。</p>
<p>对于一个人工智能专家来说，Transformer的实际定义将会因生态系统而异。让我们通过几个例子来说明：</p>
<ul class="simple">
<li><p>API：OpenAI
API不需要AI开发人员。一个网页设计师可以创建一个表单，一个语言学家或领域专家可以准备提示输入文本。一个AI专家的主要角色将需要语言技巧，以展示而不仅仅是告诉GPT-3引擎如何完成任务。例如，展示涉及对输入的上下文进行处理。这项新任务被称为提示工程。一个提示工程师在人工智能领域有着广阔的前景！</p></li>
<li><p>API驱动的软件库：Google
Trax库需要一定程度的开发工作来使用现成的模型。精通语言学和自然语言处理任务的AI专家可以处理数据集和输出。</p></li>
<li><p>训练和微调：Hugging
Face的一些功能需要一定程度的开发，提供API和库。然而，在某些情况下，我们仍然需要亲自动手。在这种情况下，训练、微调模型和找到正确的超参数将需要人工智能专家的专业知识。</p></li>
<li><p>开发级技能：在一些项目中，分词器和数据集可能不匹配，如“<a class="reference internal" href="../chapter_09_matching_tokenizers_and_datasets/index.html#chapter-9"><span class="std std-ref">分词器和数据集的匹配</span></a>”中所述。在这种情况下，例如与语言学家合作的人工智能开发人员可以发挥关键作用。因此，在这个层面上，计算语言学的培训非常有用。</p></li>
</ul>
<p>最近的NLP AI发展可以称为“嵌入式Transformer”，这正在颠覆AI开发生态系统：</p>
<ul class="simple">
<li><p>GPT-3 Transformer目前嵌入在几个Microsoft Azure应用程序中，例如GitHub
Copilot。正如本章的Foundation
models部分介绍的那样，Codex是我们将在“<a class="reference internal" href="../chapter_16_the_emergence_of_transformer_driven_copilots/index.html#chapter-16"><span class="std std-ref">Transformer驱动的Copilot</span></a>”中研究的另一个例子。</p></li>
<li><p>这些嵌入式Transformer不能直接访问，但可以提供自动化开发支持，例如自动生成代码。</p></li>
<li><p>对于终端用户来说，使用嵌入式Transformer是无缝的，通过辅助文本完成来实现。</p></li>
</ul>
<p>要直接访问GPT-3引擎，您首先需要创建一个OpenAI账户。然后，您可以使用API或直接在OpenAI用户界面中运行示例。也可以通过(poe.com)[<a class="reference external" href="https://poe.com/ChatGPT">https://poe.com/ChatGPT</a>]等代理平台间接访问。</p>
</div>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">1.3. 我们需要什么资源</a><ul>
<li><a class="reference internal" href="#transformer-api">1.3.1. Transformer API</a></li>
<li><a class="reference internal" href="#api">1.3.2. 基于API的软件库</a></li>
<li><a class="reference internal" href="#transformer">1.3.3. 预训练Transformer模型</a></li>
<li><a class="reference internal" href="#id2">1.3.4. 工业4.0时代人工智能专家的角色</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="section_02_optimizing_nlp_models_with_transformers.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>1.2. 使用Transformer优化NLP模型</div>
         </div>
     </a>
     <a id="button-next" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/index.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>2. 从Transformer的架构开始</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>