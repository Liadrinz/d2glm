
从头开始预训练
==============

在本章中，我们将使用Hugging
Face为类似BERT的模型提供的构建块来训练一个名为KantaiBERT的transformer模型。我们在第三章:ref:`chapter-3`\ 中介绍了我们将使用的模型的构建块的理论知识。

KantaiBERT是一种基于BERT架构的，类似于RoBERTa（Robustly Optimized BERT
Pretraining Approach）的模型。

原始的BERT模型为原始的transformer模型带来了创新的特性，而RoBERTa通过改进预训练过程的机制，进一步提高了transformer在下游任务中的性能。

例如，RoBERTa不使用WordPiece分词，而是采用字节级别的字节对编码（Byte-Pair
Encoding，BPE）。这种方法为各种BERT和类BERT模型铺平了道路。

在本章中，KantaiBERT和BERT一样，将使用Masked Language
Modeling（MLM）进行训练。MLM是一种语言建模技术，它会在序列中掩盖一个单词，然后transformer模型必须训练以预测被掩盖的单词。

KantaiBERT将作为一个小型模型进行训练，它由6层、12个注意力头和84,095,008个参数组成。这个参数数量似乎很多，但是这些参数分布在12个注意力头上，使得它成为一个相对较小的模型。小型模型将使预训练过程更加平滑，每个步骤都可以实时查看结果，而无需等待几个小时。

KantaiBERT是类似DistilBERT的模型，因为它具有相同的6层和12个注意力头的架构。DistilBERT意思是进行知识蒸馏（Knowledge
Distillation）后的BERT模型，其参数量比原始BERT模型少。类似地，KantaiBERT也是蒸馏后的RoBERTa模型，其参数数量比原始RoBERTa模型少。因此，它的运行速度更快，但结果略微比RoBERTa模型的精确度较低。

第1步：加载数据集
-----------------

通过使用预先准备好的数据集，我们可以客观地训练和比较transformer模型。

这里选择使用伊曼努尔·康德（1724-1804）的作品，他是启蒙时代的典范，也是一位德国哲学家。这个想法是为了在下游推理任务中引入类人的逻辑和预训练的推理能力。

`Gutenberg <https://www.gutenberg.org>`__\ 中提供了众多免费的电子书，可以下载为文本格式。你也可以使用其他书作为自定义的数据集。

这里收集了伊曼努尔·康德的以下三本书中的语料作为训练数据，并将其命名为\ ``kant.txt``:

-  纯粹理性批判（The Critique of Pure Reason）
-  实践理性批判（The Critique of Practical Reason）
-  道德形而上学基础（Fundamental Principles of the Metaphysic of
   Morals）

``kant.txt``\ 提供了一个小型训练数据集，用于训练本章的transformer模型。所得到的结果仍然是实验性的。对于一个真实需要落地的项目，可能需要添加更多的语料。

可以使用以下命令下载\ ``kant.txt``:

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    !curl -L https://raw.githubusercontent.com/Denis2054/Transformers-for-NLP-2nd-Edition/master/Chapter04/kant.txt --output "kant.txt"


.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                     Dload  Upload   Total   Spent    Left  Speed
      0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
    curl: (7) Failed to connect to raw.githubusercontent.com port 443: Connection refused


第2步：安装Hugging Face transformers
------------------------------------

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    !pip install transformers


.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple
    Requirement already satisfied: transformers in /root/miniconda3/envs/book/lib/python3.10/site-packages (4.39.2)


.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    Requirement already satisfied: filelock in /root/miniconda3/envs/book/lib/python3.10/site-packages (from transformers) (3.13.4)
    Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /root/miniconda3/envs/book/lib/python3.10/site-packages (from transformers) (0.22.2)
    Requirement already satisfied: numpy>=1.17 in /root/miniconda3/envs/book/lib/python3.10/site-packages (from transformers) (1.26.4)
    Requirement already satisfied: packaging>=20.0 in /root/miniconda3/envs/book/lib/python3.10/site-packages (from transformers) (24.0)
    Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/envs/book/lib/python3.10/site-packages (from transformers) (6.0.1)
    Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/envs/book/lib/python3.10/site-packages (from transformers) (2023.12.25)
    Requirement already satisfied: requests in /root/miniconda3/envs/book/lib/python3.10/site-packages (from transformers) (2.31.0)
    Requirement already satisfied: tokenizers<0.19,>=0.14 in /root/miniconda3/envs/book/lib/python3.10/site-packages (from transformers) (0.15.2)
    Requirement already satisfied: safetensors>=0.4.1 in /root/miniconda3/envs/book/lib/python3.10/site-packages (from transformers) (0.4.2)
    Requirement already satisfied: tqdm>=4.27 in /root/miniconda3/envs/book/lib/python3.10/site-packages (from transformers) (4.66.2)


.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/envs/book/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)
    Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/envs/book/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)


.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/book/lib/python3.10/site-packages (from requests->transformers) (3.3.2)
    Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/book/lib/python3.10/site-packages (from requests->transformers) (2.10)
    Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/book/lib/python3.10/site-packages (from requests->transformers) (2.2.1)
    Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/book/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)


.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
    

第3步：训练分词器（tokenizer）
------------------------------

在本节中，我们不使用预训练的分词器（如GPT-2的预训练分词器），而是从头开始训练一个分词器。

我们将使用\ ``kant.txt``\ 来训练Hugging
Face的\ ``ByteLevelBPETokenizer``. 一个BPE（Byte-Pair
Encoding）分词器会将一个字符串或单词分解为子字符串或子词。除了其他许多优点外，这种方法有两个主要优势：

-  分词器可以将单词分解为最小的组成部分，然后将这些小组件合并为统计上有趣的组合。例如，“smaller”和“smallest”可以变为“small”，“er”和“est”。分词器还可以进一步拆分，例如，我们可以得到“sm”和“all”。无论如何，单词都被拆分为子词标记和更小的子词部分，例如“sm”和“all”，而不是简单的“small”。
-  使用WordPiece级别的编码，被分类为未知（unk_token）的字符串块将基本上消失。

在这个模型中，我们将使用以下参数来训练分词器：

-  ``files=/path/to/dataset``\ 是数据集的路径
-  ``vocab_size=52000``\ 是词表大小
-  ``min_frequency=2``\ 是最小频率的阈值
-  ``special_tokens=[]``\ 是特殊token的列表

这里，特殊token包括：

-  ``<s>``: 开始token
-  ``<pad>``: 填充token
-  ``</s>``: 结束token
-  ``<unk>``: 未知token
-  ``<mask>``: 用于MLM的掩码token

我们来看某个句子中的两个单词：

::

   ...the tokenizer...

分词器的第一步是将字符串变为tokens:

::

   'Ġthe', 'Ġtoken', 'izer',

其中\ ``Ġ``\ 表示空格信息。

下一步是使用token的索引来代替token:

====== ======== ======
‘Ġthe’ ‘Ġtoken’ ‘izer’
====== ======== ======
150    5430     4712
====== ======== ======

整个流程代码如下：

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    %%time
    from pathlib import Path
    from tokenizers import ByteLevelBPETokenizer
    
    paths = [str(x) for x in Path(".").glob("**/*.txt")]
    # 初始化一个分词器
    tokenizer = ByteLevelBPETokenizer()
    # 自定义训练
    tokenizer.train(
        files=paths,
        vocab_size=52000,
        min_frequency=2,
        special_tokens=[
            "<s>",
            "<pad>",
            "</s>",
            "<unk>",
            "<mask>",
        ]
    )


.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    
    


.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    
    CPU times: user 13.6 s, sys: 8.37 s, total: 22 s
    Wall time: 3.57 s


可以看到，上述代码输出了训练运行的时间。现在，tokenizer被训练好了，我们准备把tokenizer保存下来。

第4步：将分词器保存到磁盘
-------------------------

分词器被训练后会产生两个文件：

-  ``merges.txt``: 包含了合并的标记化子字符串（tokenized substrings）。
-  ``vocab.json``: 包含了标记化子串的索引

以下代码将上述代码所训练的tokenizer对应的两个文件保存到\ ``KantaiBERT``\ 目录下：

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    import os
    
    os.makedirs("KantaiBERT", exist_ok=True)
    tokenizer.save_model("KantaiBERT")




.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    ['KantaiBERT/vocab.json', 'KantaiBERT/merges.txt']



在这个例子中，文件的大小较小。您可以双击打开这些文件来查看它们的内容。可以看到\ ``merges.txt``\ 包含了标记化的子字符串：

::

   #version: 0.2 - Trained by 'huggingface/tokenizers'
   Ġ t
   h e
   Ġ a
   o n
   i n
   Ġ o
   Ġt he
   r e
   i t
   Ġo f

``vocab.json``\ 包含了token的索引：

::

   […,"Ġthink":955,"preme":956,"ĠE":957,"Ġout":958,"Ġdut":959, "aly":960,"Ġexp":961,…]

第5步：加载训练的分词器
-----------------------

我们本来可以直接加载Hugging
Face上预训练的分词器，但是现在我们可以使用我们自己训练的分词器了：

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    from tokenizers.implementations import ByteLevelBPETokenizer
    
    tokenizer = ByteLevelBPETokenizer(
    "./KantaiBERT/vocab.json",
    "./KantaiBERT/merges.txt",
    )

tokenizer可以对一个序列进行编码：

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    encoding = tokenizer.encode("The Critique of Pure Reason.")

可以看到，\ ``"The Critique of Pure Reason."``\ 这句话变成了如下token序列：

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    encoding




.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    Encoding(num_tokens=6, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])



为了将序列用于BERT模型及其变种的训练，我们需要让tokenizer对传入的序列进行后处理，加上开始token和结束token:

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    from tokenizers.processors import BertProcessing
    
    # 后处理
    tokenizer._tokenizer.post_process = BertProcessing(
        ("</s>", tokenizer.token_to_id("</s>")),
        ("<s>", tokenizer.token_to_id("<s>")),
    )
    # 截断处理
    tokenizer.enable_truncation(max_length=512)

这样一来，再次调用tokenizer进行分词时，会得到包含特殊token的token序列：

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    tokenizer.encode("The Critique of Pure Reason.").tokens




.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    ['The', 'ĠCritique', 'Ġof', 'ĠPure', 'ĠReason', '.']



第6步：检查资源：GPU和CUDA
--------------------------

使用以下命令检查GPU:

::

   !nvidia-smi

以上命令会输出GPU的型号和显存等信息。

使用以下python代码检查cuda是否可用：

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    import torch
    
    torch.cuda.is_available()




.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    False



输出为\ ``True``\ 说明cuda可用。

这里建议使用\ `Google
Colab <https://colab.research.google.com/>`__\ 或\ `Kaggle
Notebook <https://www.kaggle.com/code>`__\ ，这样可以确保GPU和cuda是可用的。

如果你使用自己的笔记本或者自己搭建的服务器上的GPU，请先安装CUDA和cuDNN（Google/问GPT）.

第7步：定义模型的配置
---------------------

我们将使用与DistilBERT
transformer相同数量的层数和注意力头数，对一个RoBERTa类型的transformer模型进行预训练。该模型的词汇表大小将设置为52,000，具有12个注意力头和6个层：

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    from transformers import RobertaConfig
    
    config = RobertaConfig(
        vocab_size=52_000,
        max_position_embeddings=514,
        num_attention_heads=12,
        num_hidden_layers=6,
        type_vocab_size=1,
    )

我们会在后续步骤中详细探索这些配置。

第8步：在transformers中重新加载tokenizer
----------------------------------------

我们现在准备好加载我们训练好的分词器，即使用RobertaTokenizer.from_pretained()加载我们的预训练分词器：

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    from transformers import RobertaTokenizer
    
    tokenizer = RobertaTokenizer.from_pretrained("./KantaiBERT", max_length=512)

第9步：从头初始化模型
---------------------

首先导入用于掩码语言建模（MLM）的RoBERTa模型：

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    from transformers import RobertaForMaskedLM

使用上面创建的配置来初始化模型：

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    model = RobertaForMaskedLM(config=config)

打印模型可以看到，它是一个有6层和12个注意力头的BERT模型：

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    print(model)


.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    RobertaForMaskedLM(
      (roberta): RobertaModel(
        (embeddings): RobertaEmbeddings(
          (word_embeddings): Embedding(52000, 768, padding_idx=1)
          (position_embeddings): Embedding(514, 768, padding_idx=1)
          (token_type_embeddings): Embedding(1, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): RobertaEncoder(
          (layer): ModuleList(
            (0-5): 6 x RobertaLayer(
              (attention): RobertaAttention(
                (self): RobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): RobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): RobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): RobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (lm_head): RobertaLMHead(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (decoder): Linear(in_features=768, out_features=52000, bias=True)
      )
    )


在继续之前，请花些时间仔细研究上述模型输出的细节。这样您将能够从内部了解模型。

探索模型参数
~~~~~~~~~~~~

上面的模型算规模较小的，共有84,095,008个参数，可以用代码查看其参数量：

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    print(model.num_parameters())


.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    83504416


代码打印的参数量只是一个大致的数，不同transformers版本之间可能有细微的差别。

现在让我们来看看参数。我们首先将参数存储在\ ``LP``\ 中，并计算参数列表的长度：

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    LP=list(model.parameters())
    lp=len(LP)
    print(lp)


.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    106


输出显示，模型的参数中大约有108个矩阵和向量。这一数字在不同transformers版本之间也可能有不同。

可以打印每个矩阵和向量来查看参数的详情：

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

   for p in range(0, lp):
       print(LP[p])

模型总参数量的计算方法是，把里面每一个向量和矩阵所包含的元素的数量加起来。

第10步：构建数据集
------------------

现在我们将逐行加载数据集，以生成用于批量训练的样本：

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    %%time
    from transformers import LineByLineTextDataset
    
    dataset = LineByLineTextDataset(
    tokenizer=tokenizer,
    file_path="./kant.txt",
    block_size=128,
    )


.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    2024-05-14 12:42:36.220818: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
    2024-05-14 12:42:36.379046: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
    To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.


.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    2024-05-14 12:42:37.093493: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT


.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    CPU times: user 15.7 s, sys: 1.5 s, total: 17.2 s
    Wall time: 15.6 s


其中\ ``block_size = 128``\ 限制了一条数据的长度。输出显示，Hugging
Face在优化数据处理时间方面投入了相当多的资源。墙时（wall
time），即处理器实际活动的时间，已经得到了优化。

第11步：定义数据整合器
----------------------

在初始化训练器之前，我们需要运行一个数据整合器（data
collator）。数据整合器会从数据集中获取样本并将它们整合成批次。其结果是类似字典的对象。

通过设置数据整合器的参数\ ``mlm=True``\ ，我们可以将数据处理为MLM训练所需的形式，整合器将会在文本中加入掩码token
``<mask>``.

我们还设置了训练MLM的掩码标记数量，即\ ``mlm_probability=0.15``\ ，表示每个token有15%的概率被掩蔽掉。

现在我们使用我们的分词器初始化数据整合器，激活MLM，并将掩码标记的比例设置为0.15：

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    from transformers import DataCollatorForLanguageModeling
    
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer, mlm=True, mlm_probability=0.15
    )

第12步：初始化训练器
--------------------

前面的步骤已经准备好了初始化训练器所需的信息：数据集、分词器、模型以及数据整合器。

现在我们可以初始化训练器了。由于只是为了演示，我们将对模型进行快速训练，训练的轮数被限制为一轮。由于我们可以共享批次并进行多进程训练任务，所以GPU非常有用：

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    from transformers import Trainer, TrainingArguments
    
    training_args = TrainingArguments(
        output_dir="./KantaiBERT",
        overwrite_output_dir=True,
        num_train_epochs=1,
        per_device_train_batch_size=64,
        save_steps=10000,
        save_total_limit=2,
    )
    trainer = Trainer(
        model=model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=dataset,
    )


.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    /root/miniconda3/envs/book/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32


现在模型准备好进行训练了。

第13步：预训练模型
------------------

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

   %%time
   trainer.train()

输出实时显示训练过程，包括损失（loss）、学习率（learning
rate）、轮数（epoch）和训练步数（steps）：

第14步：将最终的模型（包括分词器和配置）保存到磁盘上
----------------------------------------------------

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

   trainer.save("./KantaiBERT")

可以在文件系统中看到，\ ``KantaiBERT``\ 目录下多出来了\ ``config.json``,
``pytorch_model.bin``\ 和\ ``training_args.bin``\ 文件，以及与分词器相关的\ ``merges.txt``\ 和\ ``vocab.json``\ 文件。

至此，我们就从头开始构建了一个预训练模型，接下来我们构建一个流水线（pipeline）来使用这个模型。

第15步：使用\ ``FillMaskPipeline``\ 进行语言建模
------------------------------------------------

现在我们将进行填充掩码（fill-mask）任务，我们将使用我们训练过的模型和训练过的分词器来执行MLM：

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

   from transformers import pipeline
   fill_mask = pipeline(
       "fill-mask",
       model="./KantaiBERT",
       tokenizer="./KantaiBERT"
   )

现在我们可以让模型像伊曼努尔·康德一样思考：

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

   fill_mask("Human thinking involves human <mask>.")

模型会根据从数据集中学到的知识来确定\ ``<mask>``\ 应该是什么词.

后续步骤
--------

你已经从头开始训练了一个Transformer模型。花些时间想象一下，在个人或企业环境中你可以做什么。你可以为特定任务创建一个数据集，并从头开始训练它。利用你的兴趣领域或公司项目，来探索这个迷人的Transformer构建工具的世界！

一旦你训练出一个你喜欢的模型，你可以与Hugging
Face社区分享它。你的模型将出现在\ `Hugging
Face模型页面 <https://huggingface.co/models>`__

你可以按照\ `这个页面上的说明 <https://huggingface.co/transformers/model_sharing.html>`__\ ，通过几个步骤上传你的模型。

你也可以下载Hugging
Face社区分享的模型，以获取关于你个人和专业项目的新想法。
