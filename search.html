<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>Search &#8212; Transformer for NLP 0.0.1 documentation</title>

    <link rel="stylesheet" href="_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="_static/d2l.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/d2l.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="#" />
  <script type="text/javascript" src="_static/searchtools.js "></script>
  <script type="text/javascript" src="_static/language_data.js"></script>
  <script type="text/javascript">
    jQuery(function() { Search.loadIndex("searchindex.js"); });
  </script>
  
  <script type="text/javascript" id="searchindexloader"></script>
   

  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link is-active">Search</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
        
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="index.html">
              <span class="title-text">
                  Transformer for NLP
              </span>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="chapter_01_what_are_transformers/index.html">1. 什么是Transformer?</a><ul>
<li class="toctree-l2"><a class="reference internal" href="chapter_01_what_are_transformers/section_01_the_ecosystem_of_transformers.html">1.1. Transformer的生态系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_01_what_are_transformers/section_02_optimizing_nlp_models_with_transformers.html">1.2. 使用Transformer优化NLP模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_01_what_are_transformers/section_03_what_resources_should_we_use.html">1.3. 我们需要什么资源</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="chapter_02_getting_started_with_architecture_of_the_transformer_model/index.html">2. 从Transformer的架构开始</a><ul>
<li class="toctree-l2"><a class="reference internal" href="chapter_02_getting_started_with_architecture_of_the_transformer_model/section_01_the_rise_of_the_transformer_attention_is_all_you_need.html">2.1. Transformer模型架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_02_getting_started_with_architecture_of_the_transformer_model/section_02_training_and_performance.html">2.2. 模型训练和表现</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_02_getting_started_with_architecture_of_the_transformer_model/section_03_transformer_models_in_hugging_face.html">2.3. Hugging Face上的Transformer模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_02_getting_started_with_architecture_of_the_transformer_model/section_04_exercise.html">2.4. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="chapter_03_fine_tuning_bert_models/index.html">3. 微调BERT模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="chapter_03_fine_tuning_bert_models/section_01_the_architecture_of_bert.html">3.1. BERT模型的架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_03_fine_tuning_bert_models/section_02_fine_tuning_bert.html">3.2. 微调BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_03_fine_tuning_bert_models/section_03_exercise.html">3.3. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="chapter_04_pretraining_a_roberta_model_from_scratch/index.html">4. 从头开始预训练RoBERTa模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="chapter_04_pretraining_a_roberta_model_from_scratch/section_01_pretraining_from_scratch.html">4.1. 从头开始预训练</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_04_pretraining_a_roberta_model_from_scratch/section_02_exercise.html">4.2. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="chapter_05_downstream_nlp_tasks_with_transformers/index.html">5. 使用Transformer进行下游NLP任务</a><ul>
<li class="toctree-l2"><a class="reference internal" href="chapter_05_downstream_nlp_tasks_with_transformers/section_01_transformer_performances_versus_human_baselines.html">5.1. Transformer的性能 VS 人类基准</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_05_downstream_nlp_tasks_with_transformers/section_02_running_downstream_tasks.html">5.2. 运行下游任务</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_05_downstream_nlp_tasks_with_transformers/section_03_exercise.html">5.3. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="chapter_06_machine_translation_with_the_transformer/index.html">6. 基于Transformer的机器翻译</a><ul>
<li class="toctree-l2"><a class="reference internal" href="chapter_06_machine_translation_with_the_transformer/section_01_defining_machine_translation.html">6.1. 机器翻译的定义</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_06_machine_translation_with_the_transformer/section_02_preprocessing_a_wmt_dataset.html">6.2. 预处理WMT数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_06_machine_translation_with_the_transformer/section_03_evaluating_machine_translation_with_bleu.html">6.3. 使用BLEU评估机器翻译的质量</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_06_machine_translation_with_the_transformer/section_04_translations_with_trax.html">6.4. 使用Trax进行翻译</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="chapter_07_the_rise_of_suprahuman_transformers_with_gpt_3_engines/index.html">7. GPT-3的崛起</a><ul>
<li class="toctree-l2"><a class="reference internal" href="chapter_07_the_rise_of_suprahuman_transformers_with_gpt_3_engines/section_01_suprahuman_nlp_with_gpt3-transformer-models.html">7.1. 利用GPT-3进行超人类NLP任务</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_07_the_rise_of_suprahuman_transformers_with_gpt_3_engines/section_02_the_architecture_of_openai_gpt_transformer_models.html">7.2. GPT模型架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_07_the_rise_of_suprahuman_transformers_with_gpt_3_engines/section_03_generic_text_completion_with_gpt2.html">7.3. 利用GPT-2进行通用的文本补全任务</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_07_the_rise_of_suprahuman_transformers_with_gpt_3_engines/section_04_running_openai_gpt3_tasks.html">7.4. 运行GPT-3的任务</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_07_the_rise_of_suprahuman_transformers_with_gpt_3_engines/section_05_comparing_the_output_of_gpt2_and_gpt3.html">7.5. 比较GPT-2与GPT-3的输出</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_07_the_rise_of_suprahuman_transformers_with_gpt_3_engines/section_06_fine_tuning_gpt3.html">7.6. 微调GPT-3</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_07_the_rise_of_suprahuman_transformers_with_gpt_3_engines/section_07_the_role_of_an_industry_40_ai_specialist.html">7.7. 工业4.0下AI专家的角色</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="chapter_08_applying_transformers_to_legal_and_financial_documents_for_ai_text_summarization/index.html">8. T5模型解决多种NLP任务</a><ul>
<li class="toctree-l2"><a class="reference internal" href="chapter_08_applying_transformers_to_legal_and_financial_documents_for_ai_text_summarization/section_01_designing_a_universal_text_to_text_model.html">8.1. 设计一个通用的文本到文本（text-to-text）模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_08_applying_transformers_to_legal_and_financial_documents_for_ai_text_summarization/section_02_text_summarization_with_t5.html">8.2. 用T5做文本摘要</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="chapter_09_matching_tokenizers_and_datasets/index.html">9. 分词器和数据集的匹配</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter_14_interpreting_black_box_transformer_models/index.html">10. 解释黑盒Transformer模型</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter_16_the_emergence_of_transformer_driven_copilots/index.html">11. Transformer驱动的Copilot</a></li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="index.html">
              <span class="title-text">
                  Transformer for NLP
              </span>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="chapter_01_what_are_transformers/index.html">1. 什么是Transformer?</a><ul>
<li class="toctree-l2"><a class="reference internal" href="chapter_01_what_are_transformers/section_01_the_ecosystem_of_transformers.html">1.1. Transformer的生态系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_01_what_are_transformers/section_02_optimizing_nlp_models_with_transformers.html">1.2. 使用Transformer优化NLP模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_01_what_are_transformers/section_03_what_resources_should_we_use.html">1.3. 我们需要什么资源</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="chapter_02_getting_started_with_architecture_of_the_transformer_model/index.html">2. 从Transformer的架构开始</a><ul>
<li class="toctree-l2"><a class="reference internal" href="chapter_02_getting_started_with_architecture_of_the_transformer_model/section_01_the_rise_of_the_transformer_attention_is_all_you_need.html">2.1. Transformer模型架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_02_getting_started_with_architecture_of_the_transformer_model/section_02_training_and_performance.html">2.2. 模型训练和表现</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_02_getting_started_with_architecture_of_the_transformer_model/section_03_transformer_models_in_hugging_face.html">2.3. Hugging Face上的Transformer模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_02_getting_started_with_architecture_of_the_transformer_model/section_04_exercise.html">2.4. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="chapter_03_fine_tuning_bert_models/index.html">3. 微调BERT模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="chapter_03_fine_tuning_bert_models/section_01_the_architecture_of_bert.html">3.1. BERT模型的架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_03_fine_tuning_bert_models/section_02_fine_tuning_bert.html">3.2. 微调BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_03_fine_tuning_bert_models/section_03_exercise.html">3.3. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="chapter_04_pretraining_a_roberta_model_from_scratch/index.html">4. 从头开始预训练RoBERTa模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="chapter_04_pretraining_a_roberta_model_from_scratch/section_01_pretraining_from_scratch.html">4.1. 从头开始预训练</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_04_pretraining_a_roberta_model_from_scratch/section_02_exercise.html">4.2. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="chapter_05_downstream_nlp_tasks_with_transformers/index.html">5. 使用Transformer进行下游NLP任务</a><ul>
<li class="toctree-l2"><a class="reference internal" href="chapter_05_downstream_nlp_tasks_with_transformers/section_01_transformer_performances_versus_human_baselines.html">5.1. Transformer的性能 VS 人类基准</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_05_downstream_nlp_tasks_with_transformers/section_02_running_downstream_tasks.html">5.2. 运行下游任务</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_05_downstream_nlp_tasks_with_transformers/section_03_exercise.html">5.3. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="chapter_06_machine_translation_with_the_transformer/index.html">6. 基于Transformer的机器翻译</a><ul>
<li class="toctree-l2"><a class="reference internal" href="chapter_06_machine_translation_with_the_transformer/section_01_defining_machine_translation.html">6.1. 机器翻译的定义</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_06_machine_translation_with_the_transformer/section_02_preprocessing_a_wmt_dataset.html">6.2. 预处理WMT数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_06_machine_translation_with_the_transformer/section_03_evaluating_machine_translation_with_bleu.html">6.3. 使用BLEU评估机器翻译的质量</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_06_machine_translation_with_the_transformer/section_04_translations_with_trax.html">6.4. 使用Trax进行翻译</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="chapter_07_the_rise_of_suprahuman_transformers_with_gpt_3_engines/index.html">7. GPT-3的崛起</a><ul>
<li class="toctree-l2"><a class="reference internal" href="chapter_07_the_rise_of_suprahuman_transformers_with_gpt_3_engines/section_01_suprahuman_nlp_with_gpt3-transformer-models.html">7.1. 利用GPT-3进行超人类NLP任务</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_07_the_rise_of_suprahuman_transformers_with_gpt_3_engines/section_02_the_architecture_of_openai_gpt_transformer_models.html">7.2. GPT模型架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_07_the_rise_of_suprahuman_transformers_with_gpt_3_engines/section_03_generic_text_completion_with_gpt2.html">7.3. 利用GPT-2进行通用的文本补全任务</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_07_the_rise_of_suprahuman_transformers_with_gpt_3_engines/section_04_running_openai_gpt3_tasks.html">7.4. 运行GPT-3的任务</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_07_the_rise_of_suprahuman_transformers_with_gpt_3_engines/section_05_comparing_the_output_of_gpt2_and_gpt3.html">7.5. 比较GPT-2与GPT-3的输出</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_07_the_rise_of_suprahuman_transformers_with_gpt_3_engines/section_06_fine_tuning_gpt3.html">7.6. 微调GPT-3</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_07_the_rise_of_suprahuman_transformers_with_gpt_3_engines/section_07_the_role_of_an_industry_40_ai_specialist.html">7.7. 工业4.0下AI专家的角色</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="chapter_08_applying_transformers_to_legal_and_financial_documents_for_ai_text_summarization/index.html">8. T5模型解决多种NLP任务</a><ul>
<li class="toctree-l2"><a class="reference internal" href="chapter_08_applying_transformers_to_legal_and_financial_documents_for_ai_text_summarization/section_01_designing_a_universal_text_to_text_model.html">8.1. 设计一个通用的文本到文本（text-to-text）模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="chapter_08_applying_transformers_to_legal_and_financial_documents_for_ai_text_summarization/section_02_text_summarization_with_t5.html">8.2. 用T5做文本摘要</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="chapter_09_matching_tokenizers_and_datasets/index.html">9. 分词器和数据集的匹配</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter_14_interpreting_black_box_transformer_models/index.html">10. 解释黑盒Transformer模型</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter_16_the_emergence_of_transformer_driven_copilots/index.html">11. Transformer驱动的Copilot</a></li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <h1 id="search-documentation">Search</h1>
  <div id="fallback" class="admonition warning">
  <script type="text/javascript">$('#fallback').hide();</script>
  <p>
    Please activate JavaScript to enable the search
    functionality.
  </p>
  </div>
  <p>
    From here you can search these documents. Enter your search
    words into the box below and click "search". Note that the search
    function will automatically search for all of the words. Pages
    containing fewer words won't appear in the result list.
  </p>
  <form action="" method="get">
    <div class="mdl-textfield mdl-js-textfield mdl-textfield--floating-label">
      <input class="mdl-textfield__input" type="text" name="q" id="search-input">
      <label class="mdl-textfield__label" for="search-input">Search...</label>
    </div>
    <button type="submit" class="mdl-button mdl-js-button mdl-button--fab mdl-button--mini-fab mdl-button--colored">
      <i class="material-icons">search</i>
    </button>
    <span id="search-progress" style="padding-left: 10px"></span>
  </form>
  
  <div id="search-results">
  
  </div>

        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
  </div>
        
        </main>
    </div>
  </body>
</html>