<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>7.2. GPT模型架构 &#8212; Transformer for NLP 0.0.1 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/d2l.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="7.3. 利用GPT-2进行通用的文本补全任务" href="section_03_generic_text_completion_with_gpt2.html" />
    <link rel="prev" title="7.1. 利用GPT-3进行超人类NLP任务" href="section_01_suprahuman_nlp_with_gpt3-transformer-models.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="index.html"><span class="section-number">7. </span>GPT-3的崛起</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active"><span class="section-number">7.2. </span>GPT模型架构</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_07_the_rise_of_suprahuman_transformers_with_gpt_3_engines/section_02_the_architecture_of_openai_gpt_transformer_models.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <span class="title-text">
                  Transformer for NLP
              </span>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_01_what_are_transformers/index.html">1. 什么是Transformer?</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01_what_are_transformers/section_01_the_ecosystem_of_transformers.html">1.1. Transformer的生态系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01_what_are_transformers/section_02_optimizing_nlp_models_with_transformers.html">1.2. 使用Transformer优化NLP模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01_what_are_transformers/section_03_what_resources_should_we_use.html">1.3. 我们需要什么资源</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/index.html">2. 从Transformer的架构开始</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_01_the_rise_of_the_transformer_attention_is_all_you_need.html">2.1. Transformer模型架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_02_training_and_performance.html">2.2. 模型训练和表现</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_03_transformer_models_in_hugging_face.html">2.3. Hugging Face上的Transformer模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_04_exercise.html">2.4. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/index.html">3. 微调BERT模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/section_01_the_architecture_of_bert.html">3.1. BERT模型的架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/section_02_fine_tuning_bert.html">3.2. 微调BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/section_03_exercise.html">3.3. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_04_pretraining_a_roberta_model_from_scratch/index.html">4. 从头开始预训练RoBERTa模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04_pretraining_a_roberta_model_from_scratch/section_01_pretraining_from_scratch.html">4.1. 从头开始预训练</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04_pretraining_a_roberta_model_from_scratch/section_02_exercise.html">4.2. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/index.html">5. 使用Transformer进行下游NLP任务</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/section_01_transformer_performances_versus_human_baselines.html">5.1. Transformer的性能 VS 人类基准</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/section_02_running_downstream_tasks.html">5.2. 运行下游任务</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/section_03_exercise.html">5.3. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_06_machine_translation_with_the_transformer/index.html">6. 基于Transformer的机器翻译</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06_machine_translation_with_the_transformer/section_01_defining_machine_translation.html">6.1. 机器翻译的定义</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06_machine_translation_with_the_transformer/section_02_preprocessing_a_wmt_dataset.html">6.2. 预处理WMT数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06_machine_translation_with_the_transformer/section_03_evaluating_machine_translation_with_bleu.html">6.3. 使用BLEU评估机器翻译的质量</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06_machine_translation_with_the_transformer/section_04_translations_with_trax.html">6.4. 使用Trax进行翻译</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">7. GPT-3的崛起</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="section_01_suprahuman_nlp_with_gpt3-transformer-models.html">7.1. 利用GPT-3进行超人类NLP任务</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">7.2. GPT模型架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_03_generic_text_completion_with_gpt2.html">7.3. 利用GPT-2进行通用的文本补全任务</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_04_training_a_custom_gpt2_language_model.html">7.4. 训练自定义的GPT-2语言模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_05_running_openai_gpt3_tasks.html">7.5. 运行GPT-3的任务</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_06_comparing_the_output_of_gpt2_and_gpt3.html">7.6. 比较GPT-2与GPT-3的输出</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_07_fine_tuning_gpt3.html">7.7. 微调GPT-3</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_08_the_role_of_an_industry_40_ai_specialist.html">7.8. 工业4.0下AI专家的角色</a></li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <span class="title-text">
                  Transformer for NLP
              </span>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_01_what_are_transformers/index.html">1. 什么是Transformer?</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01_what_are_transformers/section_01_the_ecosystem_of_transformers.html">1.1. Transformer的生态系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01_what_are_transformers/section_02_optimizing_nlp_models_with_transformers.html">1.2. 使用Transformer优化NLP模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01_what_are_transformers/section_03_what_resources_should_we_use.html">1.3. 我们需要什么资源</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/index.html">2. 从Transformer的架构开始</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_01_the_rise_of_the_transformer_attention_is_all_you_need.html">2.1. Transformer模型架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_02_training_and_performance.html">2.2. 模型训练和表现</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_03_transformer_models_in_hugging_face.html">2.3. Hugging Face上的Transformer模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_04_exercise.html">2.4. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/index.html">3. 微调BERT模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/section_01_the_architecture_of_bert.html">3.1. BERT模型的架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/section_02_fine_tuning_bert.html">3.2. 微调BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/section_03_exercise.html">3.3. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_04_pretraining_a_roberta_model_from_scratch/index.html">4. 从头开始预训练RoBERTa模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04_pretraining_a_roberta_model_from_scratch/section_01_pretraining_from_scratch.html">4.1. 从头开始预训练</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04_pretraining_a_roberta_model_from_scratch/section_02_exercise.html">4.2. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/index.html">5. 使用Transformer进行下游NLP任务</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/section_01_transformer_performances_versus_human_baselines.html">5.1. Transformer的性能 VS 人类基准</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/section_02_running_downstream_tasks.html">5.2. 运行下游任务</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/section_03_exercise.html">5.3. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_06_machine_translation_with_the_transformer/index.html">6. 基于Transformer的机器翻译</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06_machine_translation_with_the_transformer/section_01_defining_machine_translation.html">6.1. 机器翻译的定义</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06_machine_translation_with_the_transformer/section_02_preprocessing_a_wmt_dataset.html">6.2. 预处理WMT数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06_machine_translation_with_the_transformer/section_03_evaluating_machine_translation_with_bleu.html">6.3. 使用BLEU评估机器翻译的质量</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06_machine_translation_with_the_transformer/section_04_translations_with_trax.html">6.4. 使用Trax进行翻译</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">7. GPT-3的崛起</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="section_01_suprahuman_nlp_with_gpt3-transformer-models.html">7.1. 利用GPT-3进行超人类NLP任务</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">7.2. GPT模型架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_03_generic_text_completion_with_gpt2.html">7.3. 利用GPT-2进行通用的文本补全任务</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_04_training_a_custom_gpt2_language_model.html">7.4. 训练自定义的GPT-2语言模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_05_running_openai_gpt3_tasks.html">7.5. 运行GPT-3的任务</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_06_comparing_the_output_of_gpt2_and_gpt3.html">7.6. 比较GPT-2与GPT-3的输出</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_07_fine_tuning_gpt3.html">7.7. 微调GPT-3</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_08_the_role_of_an_industry_40_ai_specialist.html">7.8. 工业4.0下AI专家的角色</a></li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <div class="section" id="gpt">
<h1><span class="section-number">7.2. </span>GPT模型架构<a class="headerlink" href="#gpt" title="Permalink to this heading">¶</a></h1>
<p>Transformer模型在2017年底到2020年初的短短3年内,经历了从训练到微调(fine-tuning),再到零样本(zero-shot)模型的发展。一个零样本的GPT-3
Transformer模型不需要进行微调。训练好的模型参数不需要为下游的多任务进行更新,这为自然语言处理/理解任务开启了一个全新的时代。</p>
<p>在这一节中,我们首先将了解OpenAI团队设计GPT模型的动机。我们将从零样本模型的微调开始学习。接着我们将看到如何对Transformer模型进行调试,生成令人震惊的文本补全。最后,我们将探索GPT模型的架构。</p>
<div class="section" id="transformer">
<h2><span class="section-number">7.2.1. </span>十亿级参数Transformer模型的崛起<a class="headerlink" href="#transformer" title="Permalink to this heading">¶</a></h2>
<p>Transformer模型从小型的、针对自然语言处理任务训练的模型,发展到需要很少甚至不需要微调的模型,这个过程速度之快令人咋舌。</p>
<p><a class="reference external" href="https://arxiv.org/pdf/1706.03762">Vaswani等人(2017年)</a>提出了Transformer,它在BLEU任务上超越了CNN和RNN。<a class="reference external" href="https://www.mikecaptain.com/resources/pdf/GPT-1.pdf">Radford等人(2018年)</a>提出了生成式预训练(GPT)模型,可以通过微调来完成下游任务。<a class="reference external" href="https://arxiv.org/pdf/1810.04805.pdf">Devlin等人(2019年)</a>则完善了BERT模型的微调技术。<a class="reference external" href="https://insightcivic.s3.us-east-1.amazonaws.com/language-models.pdf">Radford等人(2019年)</a>在此基础上进一步推出了GPT-2模型。</p>
<p><a class="reference external" href="https://arxiv.org/pdf/2005.14165">Brown等人(2020)</a>定义了一种不需要微调的GPT-3零样本(zero-shot)Transformer方法!</p>
<p>与此同时,<a class="reference external" href="https://arxiv.org/abs/1804.07461">Wang等人（2019年）</a>创建了GLUE基准测试来评估自然语言处理模型。但Transformer模型进化速度如此之快,已经超越了人类基准线!</p>
<p><a class="reference external" href="https://arxiv.org/abs/1905.00537">Wang等人(2019年、2020年)</a>迅速创建了SuperGLUE,将人类基准线大幅提高,使自然语言理解/处理任务变得更加具有挑战性。Transformer模型正在飞速进步,有些模型已经在撰写本文时超越了SuperGLUE排行榜上的人类基准线。</p>
<p>这是如何发生得如此之快的?我们将从模型规模这个方面来了解这一演化过程。</p>
</div>
<div class="section" id="id1">
<h2><span class="section-number">7.2.2. </span>Transformer模型规模的增长<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h2>
<p>单单从2017年到2020年这段时间,模型参数数量就从最初的Transformer模型的65M参数增加到了GPT-3模型的175B参数,如下表所示:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 27%" />
<col style="width: 61%" />
<col style="width: 12%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>模型</p></th>
<th class="head"><p>论文</p></th>
<th class="head"><p>参数量</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Transformer Base</p></td>
<td><dl class="simple">
<dt><a href="#id2"><span class="problematic" id="id3">`</span></a>Vaswani等人(2017年)</dt><dd><p>&lt;<a class="reference external" href="https://arxiv.org/pdf/1706.03762">https://arxiv.org/pdf/1706.03762</a>&gt;`__</p>
</dd>
</dl>
</td>
<td><p>65M</p></td>
</tr>
<tr class="row-odd"><td><p>Transformer Big</p></td>
<td><dl class="simple">
<dt><a href="#id4"><span class="problematic" id="id5">`</span></a>Vaswani等人(2017年)</dt><dd><p>&lt;<a class="reference external" href="https://arxiv.org/pdf/1706.03762">https://arxiv.org/pdf/1706.03762</a>&gt;`__</p>
</dd>
</dl>
</td>
<td><p>213M</p></td>
</tr>
<tr class="row-even"><td><p>BERT-Base</p></td>
<td><p><a class="reference external" href="https://arxiv.org/pdf/1810.04805.pdf">Devlin等人(2019年)</a></p></td>
<td><p>110M</p></td>
</tr>
<tr class="row-odd"><td><p>BERT-Large</p></td>
<td><p><a class="reference external" href="https://arxiv.org/pdf/1810.04805.pdf">Devlin等人(2019年)</a></p></td>
<td><p>340M</p></td>
</tr>
<tr class="row-even"><td><p>GPT-2</p></td>
<td><p><a class="reference external" href="https://insightcivic.s3.us-east-1.amazonaws.com/language-models.pdf">Radford等人(2019
年)</a></p></td>
<td><p>117M</p></td>
</tr>
<tr class="row-odd"><td><p>GPT-2</p></td>
<td><p><a class="reference external" href="https://insightcivic.s3.us-east-1.amazonaws.com/language-models.pdf">Radford等人(2019
年)</a></p></td>
<td><p>345M</p></td>
</tr>
<tr class="row-even"><td><p>GPT-2</p></td>
<td><p><a class="reference external" href="https://insightcivic.s3.us-east-1.amazonaws.com/language-models.pdf">Radford等人(2019
年)</a></p></td>
<td><p>1.5B</p></td>
</tr>
<tr class="row-odd"><td><p>GPT-3</p></td>
<td><dl class="simple">
<dt><a href="#id6"><span class="problematic" id="id7">`</span></a>Brown等人(2020)</dt><dd><p>&lt;<a class="reference external" href="https://arxiv.org/pdf/2005.14165">https://arxiv.org/pdf/2005.14165</a>&gt;`__</p>
</dd>
</dl>
</td>
<td><p>175B</p></td>
</tr>
</tbody>
</table>
<p>上表中只列出了在那段短暂时期内设计的主要模型。论文发表的日期在模型实际设计完成之后。同时,作者们也对这些论文进行了更新。</p>
<p>例如,当最初的Transformer模型掀起市场热潮后,Google
Brain、OpenAI和Facebook
AI等机构都陆续推出了新的transformer模型。此外,一些GPT-2模型的参数量还大于较小的GPT-3模型。比如,GPT-3
Small模型只有125M参数,小于345M参数的某些GPT-2模型。</p>
<p>模型架构在同一时期也在不断发展:</p>
<ul class="simple">
<li><p>模型的层数从最初Transformer的6层增加到了GPT-3的96层</p></li>
<li><p>每层的注意力头数从Transformer的8个增加到了GPT-3的96个</p></li>
<li><p>上下文长度从Transformer的512个token增加到了GPT-3的12,288个</p></li>
<li><p>模型架构规模的扩大,解释了为什么拥有96层的GPT-3
175B产生的结果比只有40层的GPT-2
1,542M更为出色。两个模型的参数量虽然可比,但层数已经翻了一番。</p></li>
</ul>
<p>让我们关注一下上下文长度,以了解transformer快速进化的另一个方面。</p>
<div class="section" id="id8">
<h3><span class="section-number">7.2.2.1. </span>上下文长度和最大路径长度<a class="headerlink" href="#id8" title="Permalink to this heading">¶</a></h3>
<p>Transformer模型的基石在于注意力子层。相应地,注意力子层的关键属性是用于处理上下文大小的方法。</p>
<p>上下文大小是人类和机器学习语言的主要方式之一。上下文大小越大,我们就越能理解呈现给我们的序列。</p>
<p>然而,随着上下文的增加，理解一个词所需要的距离也会增加。分析这种长期依赖关系也是从递归机制转变为注意力机制的重要动机之一。</p>
<p>例如，在下面的句子中，“it”这个单词与其所指代的事物“house”就相距很远：</p>
<p>“Our <em>house</em> was too small to fit a big couch, a large table, and other
furniture we would have liked in such a tiny space. We thought about
staying for some time, but finally, we decided to sell <em>it</em>.”</p>
<p>“it”的含义只有在我们追溯到句子开头的“house”这个词时才能解释清楚。对于机器来说,这是一个相当漫长的路径!</p>
<p>RNN中的循环层需要逐步存储上下文的总长度。最大路径长度就是上下文大小。处理GPT-3模型上下文大小的RNN的最大长度大小将长0(n)倍。此外,RNN无法将上下文分成多个个头并在并行化的机器架构上运行,在多个个GPU上分散操作。而注意力机制将这一操作降低到了一对一的token操作，无论两个单词在序列中距离多远，它们都会在计算注意力时被同时考虑，因此最大路径长度为<span class="math notranslate nohighlight">\(O(1)\)</span>.</p>
<p>另外，由于Transformer中注意力机制的每一层都是相同的，这使得Transformer模型规模的扩展变得很方便。</p>
</div>
</div>
<div class="section" id="zero-shot">
<h2><span class="section-number">7.2.3. </span>从微调到零样本（Zero-shot）模型<a class="headerlink" href="#zero-shot" title="Permalink to this heading">¶</a></h2>
<p>从一开始,由Radford等人(2018年)领导的OpenAI研究团队就想要将Transformer从训练模型带到GPT模型。他们的目标是在未标注数据上训练Transformer。让注意力层从无监督数据中学习语言是一个聪明的举措。</p>
<p>OpenAI决定训练Transformer去学习语言,而不是教它们执行特定的自然语言处理任务。他们想创造一个与任务无关的模型。所以他们开始在原始数据上训练Transformer模型,而不是依赖专家标注的数据。标注数据是耗时的,并且会显著减慢Transformer的训练过程。</p>
<p>OpenAI选择了论文中提到的仅使用解码器的（Decoder-only）Transformer架构，其结果的指标令达到了同行自然语言处理研究实验室最佳模型的水平。</p>
<p>Radford等人(2019年)基于GPT的Transformer模型初版取得的可喜结果,并且很快推出了零样本迁移模型。他们的核心理念是继续训练GPT模型,让它们从原始文本中学习。</p>
<p>OpenAI进一步推进研究，<a class="reference external" href="https://arxiv.org/pdf/2005.14165">Brown等人(2020)</a>基于这样的假设,即可以深入训练条件概率Transformer模型,并能够在很少或几乎不需要微调的情况下,对下游任务产生出色的结果。最终实现了在预训练模型上直接运行下游任务而无需进一步微调。这种令人惊叹的进展可以概括为四个阶段:</p>
<ul class="simple">
<li><p>微调(Fine-Tuning,
FT)是指我们在前几章探讨的方式。训练一个Transformer模型,然后在下游任务上进行微调。Radford等人(2018)设计了许多微调任务。OpenAI团队接下来逐步将任务数量减少到
0。</p></li>
<li><p>少样本(Few-Shot,
FS)代表了巨大的进步。GPT模型经过训练。当需要进行推理时,会呈现任务的示范作为条件。条件取代了权重更新,这是GPT团队从过程中排除的。我们将在本章的笔记本中,通过提供的上下文对模型进行条件设置,以获得文本完成。</p></li>
<li><p>单样本(One-Shot,
1S)进一步推进了这一过程。训练好的GPT模型仅被呈现一个下游任务的示范。同样也不允许进行权重更新。</p></li>
<li><p>零样本(Zero-Shot,
ZS)是最终目标。训练好的GPT模型不被呈现任何下游任务的示范。</p></li>
</ul>
<p>这些方法各有不同的效率水平。OpenAI
GPT团队一直在努力产出这些领先水平的Transformer模型。</p>
<p>我们现在可以解释GPT模型架构的动机:</p>
<ul class="simple">
<li><p>通过广泛的训练,教会Transformer模型如何学习一种语言。</p></li>
<li><p>专注于通过上下文条件化进行语言建模。</p></li>
<li><p>Transformer以一种全新的方式利用上下文生成文本完成。它不会消耗资源在学习下游任务上,而是致力于理解输入并做出推断,不管任务是什么。</p></li>
<li><p>GPT只使用了Transformer解码器，解码器中的掩蔽多头注意力使Transformer只能看到上文不能看到下文，迫使Transformer以机器智能的方式思考。</p></li>
</ul>
</div>
<div class="section" id="id9">
<h2><span class="section-number">7.2.4. </span>GPT解码器架构<a class="headerlink" href="#id9" title="Permalink to this heading">¶</a></h2>
<p>GPT模型与Vaswani等人(2017)设计的原始Transformer有相同的解码器堆栈结构。可以回顾:ref:<cite>ch2-sec1-decoder</cite>。</p>
<p>GPT模型只使用了解码器，其架构如 <a class="reference internal" href="#fig-ch7-sec2-fig1"><span class="std std-numref">Fig. 7.2.1</span></a> 所示：</p>
<div class="figure align-default" id="id10">
<span id="fig-ch7-sec2-fig1"></span><img alt="../_images/GPT-arch.svg" src="../_images/GPT-arch.svg" /><p class="caption"><span class="caption-number">Fig. 7.2.1 </span><span class="caption-text">GPT的decoder-only架构</span><a class="headerlink" href="#id10" title="Permalink to this image">¶</a></p>
</div>
<p>我们可以识别出GPT模型中包含以下组件:</p>
<ul class="simple">
<li><p>文本和位置嵌入子层</p></li>
<li><p>遮蔽多头自注意力层</p></li>
<li><p>归一化子层</p></li>
<li><p>前馈子层</p></li>
<li><p>输出层</p></li>
</ul>
<p>此外,还有一个 GPT-2 版本同时具备文本预测和任务分类的功能。</p>
<p>另外，与原始Transformer的解码器相比，GPT的架构中少了交叉注意力子层。这是因为GPT中没有编码器，不需要计算对编码器输出特征的注意力。</p>
<p>OpenAI团队一直在定制和微调解码器模型。Radford等人(2019)提出了不少于四个GPT模型,Brown等人(2020)也描述了不少于八个模型。通过不断的定制和优化,他们推动了GPT模型的发展。</p>
<p>GPT-3
175B模型已经达到了一个独特的规模,需要的计算资源是全世界只有少数团队能拥有的：</p>
<div class="math notranslate nohighlight" id="equation-chapter-07-the-rise-of-suprahuman-transformers-with-gpt-3-engines-section-02-the-architecture-of-openai-gpt-transformer-models-0">
<span class="eqno">(7.2.1)<a class="headerlink" href="#equation-chapter-07-the-rise-of-suprahuman-transformers-with-gpt-3-engines-section-02-the-architecture-of-openai-gpt-transformer-models-0" title="Permalink to this equation">¶</a></span>\[n_\text{params}=175.0B, n_\text{layers}=96, d_\text{model}=12288, n_\text{heads}=96\]</div>
</div>
<div class="section" id="gpt-3">
<h2><span class="section-number">7.2.5. </span>GPT-3引擎<a class="headerlink" href="#gpt-3" title="Permalink to this heading">¶</a></h2>
<p>GPT-3模型可以被训练来完成不同规模的特定任务。OpenAI记录了当前可用的<a class="reference external" href="https://beta.openai.com/docs/engines">各种GPT-3引擎</a>。这些引擎覆盖了不同的规模和功能。</p>
<p>GPT-3的基础引擎系列具有不同的功能:</p>
<ul class="simple">
<li><p>Davinci引擎擅长分析复杂的意图</p></li>
<li><p>Curie引擎速度快,擅长文本摘要</p></li>
<li><p>Babbage引擎擅长语义搜索</p></li>
<li><p>Ada引擎擅长解析文本</p></li>
</ul>
<p>OpenAI 正在推出更多的引擎供市场使用:</p>
<ul class="simple">
<li><p>Instruct系列可以根据描述提供指令</p></li>
<li><p>Codex系列可以将自然语言转换为代码</p></li>
<li><p>Content filter系列可以过滤掉不安全或敏感的文本</p></li>
</ul>
</div>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">7.2. GPT模型架构</a><ul>
<li><a class="reference internal" href="#transformer">7.2.1. 十亿级参数Transformer模型的崛起</a></li>
<li><a class="reference internal" href="#id1">7.2.2. Transformer模型规模的增长</a><ul>
<li><a class="reference internal" href="#id8">7.2.2.1. 上下文长度和最大路径长度</a></li>
</ul>
</li>
<li><a class="reference internal" href="#zero-shot">7.2.3. 从微调到零样本（Zero-shot）模型</a></li>
<li><a class="reference internal" href="#id9">7.2.4. GPT解码器架构</a></li>
<li><a class="reference internal" href="#gpt-3">7.2.5. GPT-3引擎</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="section_01_suprahuman_nlp_with_gpt3-transformer-models.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>7.1. 利用GPT-3进行超人类NLP任务</div>
         </div>
     </a>
     <a id="button-next" href="section_03_generic_text_completion_with_gpt2.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>7.3. 利用GPT-2进行通用的文本补全任务</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>