<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>3.1. BERT模型的架构 &#8212; Transformer for NLP 0.0.1 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/d2l.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3.2. 微调BERT" href="section_02_fine_tuning_bert.html" />
    <link rel="prev" title="3. 微调BERT模型" href="index.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="index.html"><span class="section-number">3. </span>微调BERT模型</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active"><span class="section-number">3.1. </span>BERT模型的架构</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_03_fine_tuning_bert_models/section_01_the_architecture_of_bert.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <span class="title-text">
                  Transformer for NLP
              </span>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_01_what_are_transformers/index.html">1. 什么是Transformer?</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01_what_are_transformers/section_01_the_ecosystem_of_transformers.html">1.1. Transformer的生态系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01_what_are_transformers/section_02_optimizing_nlp_models_with_transformers.html">1.2. 使用Transformer优化NLP模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01_what_are_transformers/section_03_what_resources_should_we_use.html">1.3. 我们需要什么资源</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/index.html">2. 从Transformer的架构开始</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_01_the_rise_of_the_transformer_attention_is_all_you_need.html">2.1. Transformer模型架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_02_training_and_performance.html">2.2. 模型训练和表现</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_03_transformer_models_in_hugging_face.html">2.3. Hugging Face上的Transformer模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_04_exercise.html">2.4. 练习</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">3. 微调BERT模型</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">3.1. BERT模型的架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_02_fine_tuning_bert.html">3.2. 微调BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_03_exercise.html">3.3. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_04_pretraining_a_roberta_model_from_scratch/index.html">4. 从头开始预训练RoBERTa模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04_pretraining_a_roberta_model_from_scratch/section.html">4.1. 从头开始预训练</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/index.html">5. 使用Transformer进行下游NLP任务</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/section_01_transformer_performances_versus_human_baselines.html">5.1. Transformer的性能 VS 人类基准</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/section_02_running_downstream_tasks.html">5.2. 运行下游任务</a></li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <span class="title-text">
                  Transformer for NLP
              </span>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_01_what_are_transformers/index.html">1. 什么是Transformer?</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01_what_are_transformers/section_01_the_ecosystem_of_transformers.html">1.1. Transformer的生态系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01_what_are_transformers/section_02_optimizing_nlp_models_with_transformers.html">1.2. 使用Transformer优化NLP模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01_what_are_transformers/section_03_what_resources_should_we_use.html">1.3. 我们需要什么资源</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/index.html">2. 从Transformer的架构开始</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_01_the_rise_of_the_transformer_attention_is_all_you_need.html">2.1. Transformer模型架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_02_training_and_performance.html">2.2. 模型训练和表现</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_03_transformer_models_in_hugging_face.html">2.3. Hugging Face上的Transformer模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_04_exercise.html">2.4. 练习</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">3. 微调BERT模型</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">3.1. BERT模型的架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_02_fine_tuning_bert.html">3.2. 微调BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_03_exercise.html">3.3. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_04_pretraining_a_roberta_model_from_scratch/index.html">4. 从头开始预训练RoBERTa模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04_pretraining_a_roberta_model_from_scratch/section.html">4.1. 从头开始预训练</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/index.html">5. 使用Transformer进行下游NLP任务</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/section_01_transformer_performances_versus_human_baselines.html">5.1. Transformer的性能 VS 人类基准</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/section_02_running_downstream_tasks.html">5.2. 运行下游任务</a></li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <div class="section" id="bert">
<h1><span class="section-number">3.1. </span>BERT模型的架构<a class="headerlink" href="#bert" title="Permalink to this heading">¶</a></h1>
<p>我们不再赘述“<a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/index.html#chapter-2"><span class="std std-ref">从Transformer的架构开始</span></a>”中描述的Transformer模型的构建模块。在本节中，我们将重点关注BERT模型的与原始Transformer中不同的地方。</p>
<p>我们重点关注<a class="reference external" href="https://arxiv.org/pdf/1810.04805.pdf">Devlin等人（2018年）</a>这篇论文中设计的改进版编码器。我们首先将介绍编码器的架构，然后介绍预训练输入环境的准备工作。然后我们将描述BERT的两步训练框架：预训练和微调。</p>
<div class="section" id="id1">
<h2><span class="section-number">3.1.1. </span>BERT编码器<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h2>
<p>BERT只使用Transformer编码器而不使用Transformer解码器。</p>
<p>原始Transformer的编码器包含<span class="math notranslate nohighlight">\(N=6\)</span>层，编码器层输入输出维度为<span class="math notranslate nohighlight">\(d_\text{model}=512\)</span>，注意力头数为<span class="math notranslate nohighlight">\(m=8\)</span>，每个注意力头输出维度为<span class="math notranslate nohighlight">\(d_k=512/8=64\)</span>.</p>
<p>BERT的编码器层比原始Transformer模型更大。BERT的论文中使用了两种不同大小的BERT:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\text{BERT}_\text{BASE}\)</span>:
编码器层数<span class="math notranslate nohighlight">\(N=12\)</span>，编码器层维度<span class="math notranslate nohighlight">\(d_\text{model}=768\)</span>，也表示为<span class="math notranslate nohighlight">\(H=768\)</span>.
注意力头数量<span class="math notranslate nohighlight">\(m=12\)</span>，每个注意力头的输出<span class="math notranslate nohighlight">\(z_i\)</span>的维度仍然为<span class="math notranslate nohighlight">\(d_k=768/12=64\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{BERT}_\text{LARGE}\)</span>:
编码器层数<span class="math notranslate nohighlight">\(N=24\)</span>，编码器维度<span class="math notranslate nohighlight">\(d_\text{model}=1024\)</span>，注意力头数量<span class="math notranslate nohighlight">\(m=16\)</span>，每个注意力头的输出<span class="math notranslate nohighlight">\(z_i\)</span>的维度仍然为<span class="math notranslate nohighlight">\(d_k=1024/16=64\)</span>.</p></li>
</ul>
<p>图 <a class="reference internal" href="#fig-1"><span class="std std-numref">Fig. 3.1.1</span></a> 直观地反映了三种不同模型的大小。</p>
<div class="figure align-default" id="id4">
<span id="fig-1"></span><img alt="../_images/2024-04-08-10-57-30.png" src="../_images/2024-04-08-10-57-30.png" />
<p class="caption"><span class="caption-number">Fig. 3.1.1 </span><span class="caption-text">三种基于Transformer的模型</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</div>
<p>模型的大小和维度在BERT风格的预训练中起着重要作用。BERT模型就像人类一样，在具有更大的脑容量（模型大小和维度）和更多知识（数据）时能够产生更好的结果。学习大量数据的大型Transformer模型将更好地进行下游自然语言处理任务的预训练。</p>
<p>接下来，我们来看看BERT模型中输入嵌入和位置编码的基本方面，首先进入第一个子层。</p>
<div class="section" id="id2">
<h3><span class="section-number">3.1.1.1. </span>准备预训练输入环境<a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h3>
<p>BERT模型没有解码器层堆叠，因此它没有掩蔽多头注意力子层。也因此，与原始Transformer不同，BERT没有采用自回归的方式（即给定部分序列，预测序列的下一个token）来进行训练。</p>
<p>BERT的作者提出了双向注意力机制，使得注意力头可以同时关注从左到右和从右到左的所有单词。BERT模型的训练包含两个训练目标：掩蔽式语言建模（Masked
Language Modeling, MLM）和下一句子预测（Next Sentence Prediction,
NSP）。</p>
<p>我们首先来看掩蔽式语言建模（MLM）</p>
<div class="section" id="mlm">
<h4><span class="section-number">3.1.1.1.1. </span>掩蔽式语言建模（MLM）<a class="headerlink" href="#mlm" title="Permalink to this heading">¶</a></h4>
<p>BERT不使用自回归的训练方式，而是引入了对句子进行双向分析的方法，在句子中对一个单词进行随机遮蔽。</p>
<p>例如，假设输入句子如下：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">The</span> <span class="n">cat</span> <span class="n">sat</span> <span class="n">on</span> <span class="n">it</span> <span class="n">because</span> <span class="n">it</span> <span class="n">was</span> <span class="n">a</span> <span class="n">nice</span> <span class="n">rug</span><span class="o">.</span>
</pre></div>
</div>
<p>自回归的训练目标在预测“because”时只能看到该单词之前的部分，之后的部分被解码器中的掩蔽注意力机制所掩盖：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">The</span> <span class="n">cat</span> <span class="n">sat</span> <span class="n">on</span> <span class="n">it</span> <span class="o">&lt;</span><span class="n">masked</span> <span class="n">sequence</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>而BERT则只将所需预测的词掩盖：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">The</span> <span class="n">cat</span> <span class="n">sat</span> <span class="n">on</span> <span class="n">it</span> <span class="p">[</span><span class="n">MASK</span><span class="p">]</span> <span class="n">it</span> <span class="n">was</span> <span class="n">a</span> <span class="n">nice</span> <span class="n">rug</span><span class="o">.</span>
</pre></div>
</div>
<p>需要注意的是，BERT对输入采用了一种名为<a class="reference external" href="https://huggingface.co/learn/nlp-course/en/chapter6/6">WordPiece</a>的子词分割的分词方法（Subword
Segmentation
Tokenization），并且使用了可学习的位置编码，而不是采用固定的正弦-余弦函数。</p>
<p>现在，多头注意力子层可以看到整个输入序列，对序列运行自注意力计算，并预测被掩蔽的token.</p>
<p>BERT通过以下三种方法对输入序列中的token进行掩蔽，对于被选中进行掩蔽的token：</p>
<ul>
<li><p>有80%的概率将其替换为特殊token <code class="docutils literal notranslate"><span class="pre">[MASK]</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">The</span> <span class="n">cat</span> <span class="n">sat</span> <span class="n">on</span> <span class="n">it</span> <span class="p">[</span><span class="n">MASK</span><span class="p">]</span> <span class="n">it</span> <span class="n">was</span> <span class="n">a</span> <span class="n">nice</span> <span class="n">rug</span><span class="o">.</span>
</pre></div>
</div>
</li>
<li><p>有10%的概率将其替换为词表中的随机单词：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">The</span> <span class="n">cat</span> <span class="n">sat</span> <span class="n">on</span> <span class="n">it</span> <span class="p">[</span><span class="n">often</span><span class="p">]</span> <span class="n">it</span> <span class="n">was</span> <span class="n">a</span> <span class="n">nice</span> <span class="n">rug</span><span class="o">.</span>
</pre></div>
</div>
</li>
<li><p>有10%的概率保持原样，不进行掩蔽。示例如下：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">The</span> <span class="n">cat</span> <span class="n">sat</span> <span class="n">on</span> <span class="n">it</span> <span class="p">[</span><span class="n">because</span><span class="p">]</span> <span class="n">it</span> <span class="n">was</span> <span class="n">a</span> <span class="n">nice</span> <span class="n">rug</span><span class="o">.</span>
</pre></div>
</div>
</li>
</ul>
<p>其中后两种方法可以防止模型过拟合。</p>
<p>接下来介绍另一个训练目标：下一句子预测（NSP）</p>
</div>
<div class="section" id="nsp">
<h4><span class="section-number">3.1.1.1.2. </span>下一句子预测（NSP）<a class="headerlink" href="#nsp" title="Permalink to this heading">¶</a></h4>
<p>下一句子预测并不是预测下一个句子，而是将两个句子拼接，让BERT判断第二个句子是否是第一个句子的下一个句子。</p>
<p>在构建输入时，正样本和负样本分别占50%.
正样本通常是从数据集中选取的一对连续句子。负样本则是使用来自不同文档的序列进行创建。</p>
<p>NSP输入的构建流程如下。给定两个句子：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">The</span> <span class="n">cat</span> <span class="n">slept</span> <span class="n">on</span> <span class="n">the</span> <span class="n">rug</span><span class="o">.</span> <span class="n">It</span> <span class="n">likes</span> <span class="n">sleeping</span> <span class="nb">all</span> <span class="n">day</span><span class="o">.</span>
</pre></div>
</div>
<p>BERT会将其分词并拼接为如下形式：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">CLS</span><span class="p">]</span> <span class="n">the</span> <span class="n">cat</span> <span class="n">slept</span> <span class="n">on</span> <span class="n">the</span> <span class="n">rug</span> <span class="p">[</span><span class="n">SEP</span><span class="p">]</span> <span class="n">it</span> <span class="n">likes</span> <span class="n">sleep</span> <span class="c1">##ing all day[SEP]</span>
</pre></div>
</div>
<p>其中“##ing”就是我们前面提到的WordPiece算法的产物，它将一个单词分为多个子词；<code class="docutils literal notranslate"><span class="pre">[CLS]</span></code>和<code class="docutils literal notranslate"><span class="pre">[SEP]</span></code>又是两种不同的特殊token:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">[CLS]</span></code>是一个分类标记，添加在第一个序列的开头，用于预测第二个序列是否跟随在第一个序列之后。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">[SEP]</span></code>是一个分隔标记，用于标识一个序列的结束。</p></li>
</ul>
<p>为了区分句子A和句子B，在进行NSP时需要引入额外的编码信息，如图
<a class="reference internal" href="#fig-2"><span class="std std-numref">Fig. 3.1.2</span></a> 所示：</p>
<div class="figure align-default" id="id5">
<span id="fig-2"></span><img alt="../_images/2024-04-08-13-07-46.png" src="../_images/2024-04-08-13-07-46.png" />
<p class="caption"><span class="caption-number">Fig. 3.1.2 </span><span class="caption-text">BERT输入编码</span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</div>
<p>其中Token Embeddings对每个token做嵌入，得到token嵌入向量；Positional
Encoding对token的位置做嵌入，得到位置编码向量；这两个嵌入模块在原始Transformer架构中都有，只不过BERT中的Positional
Encoding是可学习的。</p>
<p>Sentence Embeddings（又称Segment
Embeddings，句段嵌入）用于在NSP任务中区分第一个句子和第二个句子。同一个句子内各token的句段嵌入向量是相同的。</p>
<p>BERT模型的输入嵌入和位置编码子层可以总结如下：</p>
<ul class="simple">
<li><p>输入词序列被分解成WordPiece token序列</p></li>
<li><p>为了进行掩蔽式语言建模（MLM）训练，使用<code class="docutils literal notranslate"><span class="pre">[MASK]</span></code>
token随机替换原始token</p></li>
<li><p>为了进行分类任务，在序列开头添加<code class="docutils literal notranslate"><span class="pre">[CLS]</span></code> token</p></li>
<li><p>为了进行下一句子预测（NSP）训练，使用<code class="docutils literal notranslate"><span class="pre">[SEP]</span></code>隔开两个句子</p></li>
<li><p>将句段嵌入向量（Segment
Embeddings）加到token嵌入向量中，使得句子A和句子B具有不同的句子嵌入值</p></li>
<li><p>位置编码是可学习的，不采用原始Transformer的正弦-余弦位置编码方法</p></li>
</ul>
<p>BERT还有一些额外的关键特点：</p>
<ul class="simple">
<li><p>BERT在其多头注意力子层中使用双向注意力，打开了学习和理解标记之间关系的广阔视野。</p></li>
<li><p>BERT的MLM训练目标引入了无监督嵌入的场景，使用无标签文本对预训练模型进行训练。无监督场景迫使模型在多头注意力学习过程中更加努力思考。这使得BERT能够学习语言的构建方式，并将这些知识应用于下游任务，而无需每次都进行预训练。</p></li>
<li><p>BERT同时使用了有监督学习（NSP训练目标）。</p></li>
</ul>
</div>
</div>
<div class="section" id="id3">
<h3><span class="section-number">3.1.1.2. </span>BERT模型的预训练和微调<a class="headerlink" href="#id3" title="Permalink to this heading">¶</a></h3>
<p>BERT是一个两步训练框架。其中第一步是预训练，第二步是微调，如图
<a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_01_the_rise_of_the_transformer_attention_is_all_you_need.html#fig-3"><span class="std std-numref">Fig. 2.1.3</span></a> 所示。</p>
<div class="figure align-default" id="id6">
<img alt="../_images/2024-04-08-14-33-40.png" src="../_images/2024-04-08-14-33-40.png" />
<p class="caption"><span class="caption-number">Fig. 3.1.3 </span><span class="caption-text">BERT两步训练框架</span><a class="headerlink" href="#id6" title="Permalink to this image">¶</a></p>
</div>
<p>训练一个Transformer模型可能需要几个小时，甚至几天的时间。设计模型架构、调整参数以及选择适当的数据集来训练Transformer模型都需要相当一段时间进行工程化处理。</p>
<p>BERT框架的第一步是预训练（Pre-training），可以分为两个子步骤：</p>
<ul class="simple">
<li><p>定义模型的架构：层数、头数、维度以及模型的其他构建块</p></li>
<li><p>在掩蔽式语言建模（MLM）和下一句子预测（NSP）任务上对模型进行训练</p></li>
</ul>
<p>BERT框架的第二步是微调（Fine-tuning），也可以分为两个子步骤：</p>
<ul class="simple">
<li><p>使用预训练的BERT模型的训练参数初始化所选择的下游模型。</p></li>
<li><p>针对特定的下游任务（如文本推理、问答、以及对抗生成等），对参数进行微调。</p></li>
</ul>
<p>我们已经学习了BERT架构以及其预训练和微调的框架。现在让我们来对一个BERT模型进行微调。</p>
</div>
</div>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">3.1. BERT模型的架构</a><ul>
<li><a class="reference internal" href="#id1">3.1.1. BERT编码器</a><ul>
<li><a class="reference internal" href="#id2">3.1.1.1. 准备预训练输入环境</a><ul>
<li><a class="reference internal" href="#mlm">3.1.1.1.1. 掩蔽式语言建模（MLM）</a></li>
<li><a class="reference internal" href="#nsp">3.1.1.1.2. 下一句子预测（NSP）</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id3">3.1.1.2. BERT模型的预训练和微调</a></li>
</ul>
</li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="index.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>3. 微调BERT模型</div>
         </div>
     </a>
     <a id="button-next" href="section_02_fine_tuning_bert.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>3.2. 微调BERT</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>