<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>4. 从头开始预训练RoBERTa模型 &#8212; Transformer for NLP 0.0.1 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/d2l.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="4.1. 从头开始预训练" href="section.html" />
    <link rel="prev" title="3.3. 练习" href="../chapter_03_fine_tuning_bert_models/section_03_exercise.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link is-active"><span class="section-number">4. </span>从头开始预训练RoBERTa模型</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_04_pretraining_a_roberta_model_from_scratch/index.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <span class="title-text">
                  Transformer for NLP
              </span>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_01_what_are_transformers/index.html">1. 什么是Transformer?</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01_what_are_transformers/section_01_the_ecosystem_of_transformers.html">1.1. Transformer的生态系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01_what_are_transformers/section_02_optimizing_nlp_models_with_transformers.html">1.2. 使用Transformer优化NLP模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01_what_are_transformers/section_03_what_resources_should_we_use.html">1.3. 我们需要什么资源</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/index.html">2. 从Transformer的架构开始</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_01_the_rise_of_the_transformer_attention_is_all_you_need.html">2.1. Transformer模型架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_02_training_and_performance.html">2.2. 模型训练和表现</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_03_transformer_models_in_hugging_face.html">2.3. Hugging Face上的Transformer模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_04_exercise.html">2.4. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/index.html">3. 微调BERT模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/section_01_the_architecture_of_bert.html">3.1. BERT模型的架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/section_02_fine_tuning_bert.html">3.2. 微调BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/section_03_exercise.html">3.3. 练习</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">4. 从头开始预训练RoBERTa模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="section.html">4.1. 从头开始预训练</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/index.html">5. 使用Transformer进行下游NLP任务</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/section_01_transformer_performances_versus_human_baselines.html">5.1. Transformer的性能 VS 人类基准</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/section_02_running_downstream_tasks.html">5.2. 运行下游任务</a></li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <span class="title-text">
                  Transformer for NLP
              </span>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_01_what_are_transformers/index.html">1. 什么是Transformer?</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01_what_are_transformers/section_01_the_ecosystem_of_transformers.html">1.1. Transformer的生态系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01_what_are_transformers/section_02_optimizing_nlp_models_with_transformers.html">1.2. 使用Transformer优化NLP模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01_what_are_transformers/section_03_what_resources_should_we_use.html">1.3. 我们需要什么资源</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/index.html">2. 从Transformer的架构开始</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_01_the_rise_of_the_transformer_attention_is_all_you_need.html">2.1. Transformer模型架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_02_training_and_performance.html">2.2. 模型训练和表现</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_03_transformer_models_in_hugging_face.html">2.3. Hugging Face上的Transformer模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_04_exercise.html">2.4. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/index.html">3. 微调BERT模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/section_01_the_architecture_of_bert.html">3.1. BERT模型的架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/section_02_fine_tuning_bert.html">3.2. 微调BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/section_03_exercise.html">3.3. 练习</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">4. 从头开始预训练RoBERTa模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="section.html">4.1. 从头开始预训练</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/index.html">5. 使用Transformer进行下游NLP任务</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/section_01_transformer_performances_versus_human_baselines.html">5.1. Transformer的性能 VS 人类基准</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/section_02_running_downstream_tasks.html">5.2. 运行下游任务</a></li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <div class="section" id="roberta">
<span id="chapter-4"></span><h1><span class="section-number">4. </span>从头开始预训练RoBERTa模型<a class="headerlink" href="#roberta" title="Permalink to this heading">¶</a></h1>
<p>在本章中，我们将从头开始构建一个RoBERTa模型。该模型将使用构建BERT模型所需的transformer构建块。此外，我们不会使用预训练的分词器或模型。RoBERTa模型将按照本章描述的十五个步骤进行构建。</p>
<p>我们将利用前几章中学到的transformer知识，逐步构建一个可以进行掩码语言建模的模型。在第二章:ref:<cite>chapter-2</cite>中，我们介绍了原始Transformer的构建块。在第三章:ref:<cite>chapter-3</cite>中，我们微调了一个预训练的BERT模型。本章将重点介绍如何使用基于Hugging
Face的无缝模块在Jupyter
Notebook中从头开始构建一个预训练的transformer模型。该模型名为KantaiBERT。</p>
<p>我们首先介绍如何获取KantaiBERT预训练所需要的语料库——伊曼努尔·康德（Immanuel
Kant）的图书汇编，并使用Jupyter Notebook创建自己的数据集，</p>
<p>KantaiBERT会从头开始训练自己的分词器。分词器将构建合并文件（Merge
File）和词表文件（Vocabulary File），并在预训练过程中使用它们。</p>
<p>然后我们处理数据集，初始化训练器并训练KantaiBERT模型。</p>
<p>最后，KantaiBERT使用训练好的模型执行一个实验性的下游语言建模任务，并使用伊曼努尔·康德的逻辑填充一个掩码。</p>
<p>本章包含以下内容：</p>
<ul class="simple">
<li><p>RoBERTa和DistilBERT类的模型</p></li>
<li><p>如何从头开始训练分词器</p></li>
<li><p>字节级别的字节对编码（BPE）</p></li>
<li><p>将训练好的分词器保存为文件</p></li>
<li><p>为预训练过程重新创建分词器</p></li>
<li><p>从头初始化一个RoBERTa模型</p></li>
<li><p>探索模型的配置</p></li>
<li><p>探索模型的8000万个参数</p></li>
<li><p>为训练器创建数据集</p></li>
<li><p>初始化训练器</p></li>
<li><p>预训练模型</p></li>
<li><p>保存模型</p></li>
<li><p>将模型应用到下游的MLM任务</p></li>
</ul>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="section.html">4.1. 从头开始预训练</a><ul>
<li class="toctree-l2"><a class="reference internal" href="section.html#id2">4.1.1. 第1步：加载数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="section.html#hugging-face-transformers">4.1.2. 第2步：安装Hugging Face transformers</a></li>
<li class="toctree-l2"><a class="reference internal" href="section.html#tokenizer">4.1.3. 第3步：训练分词器（tokenizer）</a></li>
<li class="toctree-l2"><a class="reference internal" href="section.html#id3">4.1.4. 第4步：将分词器保存到磁盘</a></li>
<li class="toctree-l2"><a class="reference internal" href="section.html#id4">4.1.5. 第5步：加载训练的分词器</a></li>
<li class="toctree-l2"><a class="reference internal" href="section.html#gpucuda">4.1.6. 第6步：检查资源：GPU和CUDA</a></li>
<li class="toctree-l2"><a class="reference internal" href="section.html#id5">4.1.7. 第7步：定义模型的配置</a></li>
<li class="toctree-l2"><a class="reference internal" href="section.html#transformerstokenizer">4.1.8. 第8步：在transformers中重新加载tokenizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="section.html#id6">4.1.9. 第9步：从头初始化模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="section.html#id8">4.1.10. 第10步：构建数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="section.html#id9">4.1.11. 第11步：定义数据整合器</a></li>
<li class="toctree-l2"><a class="reference internal" href="section.html#id10">4.1.12. 第12步：初始化训练器</a></li>
<li class="toctree-l2"><a class="reference internal" href="section.html#id11">4.1.13. 第13步：预训练模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="section.html#id12">4.1.14. 第14步：将最终的模型（包括分词器和配置）保存到磁盘上</a></li>
<li class="toctree-l2"><a class="reference internal" href="section.html#fillmaskpipeline">4.1.15. 第15步：使用<code class="docutils literal notranslate"><span class="pre">FillMaskPipeline</span></code>进行语言建模</a></li>
<li class="toctree-l2"><a class="reference internal" href="section.html#id13">4.1.16. 后续步骤</a></li>
<li class="toctree-l2"><a class="reference internal" href="section.html#id14">4.1.17. 练习</a></li>
</ul>
</li>
</ul>
</div>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="../chapter_03_fine_tuning_bert_models/section_03_exercise.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>3.3. 练习</div>
         </div>
     </a>
     <a id="button-next" href="section.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>4.1. 从头开始预训练</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>