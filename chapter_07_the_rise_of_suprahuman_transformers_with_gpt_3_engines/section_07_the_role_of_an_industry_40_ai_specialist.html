<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>7.7. 工业4.0下AI专家的角色 &#8212; Transformer for NLP 0.0.1 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/d2l.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="8. 将Transformers用于法律和金融文件的摘要" href="../chapter_08_applying_transformers_to_legal_and_financial_documents_for_ai_text_summarization/index.html" />
    <link rel="prev" title="7.6. 微调GPT-3" href="section_06_fine_tuning_gpt3.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="index.html"><span class="section-number">7. </span>GPT-3的崛起</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active"><span class="section-number">7.7. </span>工业4.0下AI专家的角色</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_07_the_rise_of_suprahuman_transformers_with_gpt_3_engines/section_07_the_role_of_an_industry_40_ai_specialist.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <span class="title-text">
                  Transformer for NLP
              </span>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_01_what_are_transformers/index.html">1. 什么是Transformer?</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01_what_are_transformers/section_01_the_ecosystem_of_transformers.html">1.1. Transformer的生态系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01_what_are_transformers/section_02_optimizing_nlp_models_with_transformers.html">1.2. 使用Transformer优化NLP模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01_what_are_transformers/section_03_what_resources_should_we_use.html">1.3. 我们需要什么资源</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/index.html">2. 从Transformer的架构开始</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_01_the_rise_of_the_transformer_attention_is_all_you_need.html">2.1. Transformer模型架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_02_training_and_performance.html">2.2. 模型训练和表现</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_03_transformer_models_in_hugging_face.html">2.3. Hugging Face上的Transformer模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_04_exercise.html">2.4. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/index.html">3. 微调BERT模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/section_01_the_architecture_of_bert.html">3.1. BERT模型的架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/section_02_fine_tuning_bert.html">3.2. 微调BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/section_03_exercise.html">3.3. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_04_pretraining_a_roberta_model_from_scratch/index.html">4. 从头开始预训练RoBERTa模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04_pretraining_a_roberta_model_from_scratch/section_01_pretraining_from_scratch.html">4.1. 从头开始预训练</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04_pretraining_a_roberta_model_from_scratch/section_02_exercise.html">4.2. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/index.html">5. 使用Transformer进行下游NLP任务</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/section_01_transformer_performances_versus_human_baselines.html">5.1. Transformer的性能 VS 人类基准</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/section_02_running_downstream_tasks.html">5.2. 运行下游任务</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/section_03_exercise.html">5.3. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_06_machine_translation_with_the_transformer/index.html">6. 基于Transformer的机器翻译</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06_machine_translation_with_the_transformer/section_01_defining_machine_translation.html">6.1. 机器翻译的定义</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06_machine_translation_with_the_transformer/section_02_preprocessing_a_wmt_dataset.html">6.2. 预处理WMT数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06_machine_translation_with_the_transformer/section_03_evaluating_machine_translation_with_bleu.html">6.3. 使用BLEU评估机器翻译的质量</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06_machine_translation_with_the_transformer/section_04_translations_with_trax.html">6.4. 使用Trax进行翻译</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">7. GPT-3的崛起</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="section_01_suprahuman_nlp_with_gpt3-transformer-models.html">7.1. 利用GPT-3进行超人类NLP任务</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_02_the_architecture_of_openai_gpt_transformer_models.html">7.2. GPT模型架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_03_generic_text_completion_with_gpt2.html">7.3. 利用GPT-2进行通用的文本补全任务</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_04_running_openai_gpt3_tasks.html">7.4. 运行GPT-3的任务</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_05_comparing_the_output_of_gpt2_and_gpt3.html">7.5. 比较GPT-2与GPT-3的输出</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_06_fine_tuning_gpt3.html">7.6. 微调GPT-3</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">7.7. 工业4.0下AI专家的角色</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_08_applying_transformers_to_legal_and_financial_documents_for_ai_text_summarization/index.html">8. 将Transformers用于法律和金融文件的摘要</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08_applying_transformers_to_legal_and_financial_documents_for_ai_text_summarization/section_01_designing_a_universal_text_to_text_model.html">8.1. 设计一个通用的文本到文本（text-to-text）模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08_applying_transformers_to_legal_and_financial_documents_for_ai_text_summarization/section_02_text_summarization_with_t5.html">8.2. 用T5做文本摘要</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_09_matching_tokenizers_and_datasets/index.html">9. 分词器和数据集的匹配</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_14_interpreting_black_box_transformer_models/index.html">10. 解释黑盒Transformer模型</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_16_the_emergence_of_transformer_driven_copilots/index.html">11. Transformer驱动的Copilot</a></li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <span class="title-text">
                  Transformer for NLP
              </span>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_01_what_are_transformers/index.html">1. 什么是Transformer?</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01_what_are_transformers/section_01_the_ecosystem_of_transformers.html">1.1. Transformer的生态系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01_what_are_transformers/section_02_optimizing_nlp_models_with_transformers.html">1.2. 使用Transformer优化NLP模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_01_what_are_transformers/section_03_what_resources_should_we_use.html">1.3. 我们需要什么资源</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/index.html">2. 从Transformer的架构开始</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_01_the_rise_of_the_transformer_attention_is_all_you_need.html">2.1. Transformer模型架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_02_training_and_performance.html">2.2. 模型训练和表现</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_03_transformer_models_in_hugging_face.html">2.3. Hugging Face上的Transformer模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_02_getting_started_with_architecture_of_the_transformer_model/section_04_exercise.html">2.4. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/index.html">3. 微调BERT模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/section_01_the_architecture_of_bert.html">3.1. BERT模型的架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/section_02_fine_tuning_bert.html">3.2. 微调BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_03_fine_tuning_bert_models/section_03_exercise.html">3.3. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_04_pretraining_a_roberta_model_from_scratch/index.html">4. 从头开始预训练RoBERTa模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04_pretraining_a_roberta_model_from_scratch/section_01_pretraining_from_scratch.html">4.1. 从头开始预训练</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_04_pretraining_a_roberta_model_from_scratch/section_02_exercise.html">4.2. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/index.html">5. 使用Transformer进行下游NLP任务</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/section_01_transformer_performances_versus_human_baselines.html">5.1. Transformer的性能 VS 人类基准</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/section_02_running_downstream_tasks.html">5.2. 运行下游任务</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_05_downstream_nlp_tasks_with_transformers/section_03_exercise.html">5.3. 练习</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_06_machine_translation_with_the_transformer/index.html">6. 基于Transformer的机器翻译</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06_machine_translation_with_the_transformer/section_01_defining_machine_translation.html">6.1. 机器翻译的定义</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06_machine_translation_with_the_transformer/section_02_preprocessing_a_wmt_dataset.html">6.2. 预处理WMT数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06_machine_translation_with_the_transformer/section_03_evaluating_machine_translation_with_bleu.html">6.3. 使用BLEU评估机器翻译的质量</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_06_machine_translation_with_the_transformer/section_04_translations_with_trax.html">6.4. 使用Trax进行翻译</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">7. GPT-3的崛起</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="section_01_suprahuman_nlp_with_gpt3-transformer-models.html">7.1. 利用GPT-3进行超人类NLP任务</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_02_the_architecture_of_openai_gpt_transformer_models.html">7.2. GPT模型架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_03_generic_text_completion_with_gpt2.html">7.3. 利用GPT-2进行通用的文本补全任务</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_04_running_openai_gpt3_tasks.html">7.4. 运行GPT-3的任务</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_05_comparing_the_output_of_gpt2_and_gpt3.html">7.5. 比较GPT-2与GPT-3的输出</a></li>
<li class="toctree-l2"><a class="reference internal" href="section_06_fine_tuning_gpt3.html">7.6. 微调GPT-3</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">7.7. 工业4.0下AI专家的角色</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_08_applying_transformers_to_legal_and_financial_documents_for_ai_text_summarization/index.html">8. 将Transformers用于法律和金融文件的摘要</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08_applying_transformers_to_legal_and_financial_documents_for_ai_text_summarization/section_01_designing_a_universal_text_to_text_model.html">8.1. 设计一个通用的文本到文本（text-to-text）模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_08_applying_transformers_to_legal_and_financial_documents_for_ai_text_summarization/section_02_text_summarization_with_t5.html">8.2. 用T5做文本摘要</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_09_matching_tokenizers_and_datasets/index.html">9. 分词器和数据集的匹配</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_14_interpreting_black_box_transformer_models/index.html">10. 解释黑盒Transformer模型</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_16_the_emergence_of_transformer_driven_copilots/index.html">11. Transformer驱动的Copilot</a></li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <div class="section" id="ai">
<h1><span class="section-number">7.7. </span>工业4.0下AI专家的角色<a class="headerlink" href="#ai" title="Permalink to this heading">¶</a></h1>
<p>总之,工业4.0开发人员的角色是成为一名跨学科的AI专家。开发人员、数据科学家和AI专家将逐步学习更多关于语言学、业务目标、专业知识等方面的知识。工业4.0
AI专家将以实践中的跨学科知识和经验来指导团队。</p>
<p>他们需要掌握广泛的技能和知识,不仅仅是纯粹的技术领域,还包括对行业背景、业务需求、用户需求等有深入的了解。这种全面的视角和知识结构,才能真正发挥AI技术在工业4.0中的价值和潜力。</p>
<p>在实施Transformer模型时,以下三个领域需要人类专家参与:</p>
<ul>
<li><p>道德与伦理</p>
<p>在实施类人Transformer模型时,产业4.0
AI专家需要确保执行道德和伦理的做法。例如,欧洲法规对此有严格要求,需要在必要时向用户解释自动决策过程。美国也有反歧视法律来保护公民免受自动化偏见的影响。</p>
</li>
<li><p>提示（Prompts）和响应（Responses）</p>
<p>用户和UI开发人员需要借助产业4.0
AI专家来解释如何为自然语言处理任务创建适当的提示,向Transformer模型展示如何执行任务,并验证其响应。</p>
</li>
<li><p>理解模型并对模型进行质量控制</p></li>
</ul>
<p>即使在调整了模型的超参数之后,模型仍然无法按预期运行会出现什么情况?我们将在:ref:<cite>chapter-14</cite>介绍这一情况。</p>
<div class="section" id="id1">
<h2><span class="section-number">7.7.1. </span>初步结论<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h2>
<p>初步结论可以分为两类:事实和虚构。</p>
<p>一个事实是,OpenAI拥有世界上最强大的自然语言处理服务之一。其他事实包括:</p>
<ul class="simple">
<li><p>OpenAI引擎是强大的零样本引擎,不需要搜索各种Transformer模型,不需要预训练,也不需要微调</p></li>
<li><p>用于训练模型的超级计算机是独特的</p></li>
<li><p>如果提示设计得当,我们可以获得惊人准确的响应</p></li>
<li><p>在本章中实现自然语言处理任务只需要复制粘贴,任何软件初学者都可以完成</p></li>
</ul>
<p>很多人认为人工智能将取代数据科学家和人工智能专家。这是真的吗?在回答这个问题之前,先问自己以下问题:</p>
<ul class="simple">
<li><p>我们如何知道一个句子是否不正确? •</p></li>
</ul>
<p>我们如何在不阅读和确认的情况下知道答案是否正确? •
引擎是如何知道这是一个语法纠正任务的? •
如果响应不正确,我们如何理解发生了什么,以帮助改进提示或恢复到良好设计的人机界面中的手动模式?</p>
<p>事实是,人类需要手动介入来回答这些问题,需要使用规则库、经过质量控制的自动化管道以及许多其他工具。</p>
<p>这些事实是有说服力的。在许多情况下,运行一个自然语言处理任务只需要很少的开发工作。</p>
<p>人类仍然是必需的。OpenAI引擎并不是为了取代人类,而是为了帮助人类执行更高级的有意义的任务。现在你可以驾驶喷气式飞机,而不必自己制造它!</p>
<p>我们需要回答本节中提出的令人兴奋的问题。那么,让我们继续探索您在这条通往人工智能未来的精彩道路上的全新迷人的工业4.0角色吧!</p>
</div>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">7.7. 工业4.0下AI专家的角色</a><ul>
<li><a class="reference internal" href="#id1">7.7.1. 初步结论</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="section_06_fine_tuning_gpt3.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>7.6. 微调GPT-3</div>
         </div>
     </a>
     <a id="button-next" href="../chapter_08_applying_transformers_to_legal_and_financial_documents_for_ai_text_summarization/index.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>8. 将Transformers用于法律和金融文件的摘要</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>